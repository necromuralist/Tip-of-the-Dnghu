<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tip of the Dnghu (Posts about nlp)</title><link>https://necromuralist.github.io/Tip-of-the-Dnghu/</link><description></description><atom:link href="https://necromuralist.github.io/Tip-of-the-Dnghu/categories/nlp.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2019 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Thu, 09 May 2019 20:48:04 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>The Vector Space Model</title><link>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#orgc1aefd5"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#orgb32e3f7"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#org4214af9"&gt;The Query&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#orga562cf5"&gt;The Document&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#orgdd03b34"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#org7a1c741"&gt;The Bit Vector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#org76c95d6"&gt;Term Frequency Vector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#orgf927758"&gt;Inverse Document Frequency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#org6daf1f8"&gt;Ranking with TF-IDF Weighting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#orgb61056b"&gt;Okapi BM25&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#org89a66c3"&gt;Document Length Normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#org3dd934e"&gt;The State of the Art&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#org5ef4ccd"&gt;End&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#orgb0efdd2"&gt;VSM Improvements&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc1aefd5" class="outline-2"&gt;
&lt;h2 id="orgc1aefd5"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc1aefd5"&gt;
&lt;p&gt;
This is a brief write-up of some notes I took on the Vector Space Model (VSM) used to rank documents by relevancy to keywords in a query. The query is represented as a vector with a 1 for each of the search terms and, in the simplest case, the vector to determine relevance will hold a 1 in each place where the document has a word that matches a query term. This might be easier with an example.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb32e3f7" class="outline-3"&gt;
&lt;h3 id="orgb32e3f7"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb32e3f7"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# python
from collections import Counter
from functools import partial
from pathlib import Path

# pypi
from bokeh.models import HoverTool
import holoviews
import hvplot.pandas
import numpy
import pandas

# my stuff
from graeae.visualization import EmbedHoloview
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
For plotting.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;holoviews.extension("bokeh")
SLUG = "the-vector-space-model/"
output = Path("../../files/posts/text_mining")/SLUG
Embed = partial(EmbedHoloview, folder_path=output)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4214af9" class="outline-3"&gt;
&lt;h3 id="org4214af9"&gt;The Query&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4214af9"&gt;
&lt;p&gt;
The user made a query with some key words which is normally represented by a vector with the count of each term as the values.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;terms = set("movie horror blood".split())
query = numpy.ones(len(terms))
print(terms)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
{'movie', 'horror', 'blood'}

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga562cf5" class="outline-3"&gt;
&lt;h3 id="orga562cf5"&gt;The Document&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga562cf5"&gt;
&lt;p&gt;
We want to retrieve the most relevant documents from a set of documents, which I'll represent as lists of words.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;document_1 = "the movie was about workers at a blood bank".split()
document_2 = "a movie about blood blood and more blood".split()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgdd03b34" class="outline-2"&gt;
&lt;h2 id="orgdd03b34"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgdd03b34"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7a1c741" class="outline-3"&gt;
&lt;h3 id="org7a1c741"&gt;The Bit Vector&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7a1c741"&gt;
&lt;p&gt;
To represent similarity I'll use a vector with ones where there is a word match in the document.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def check_document(document: list, terms: set) -&amp;gt; None:
    print(f"Document: {document}")
    matches = []
    matched_words = []
    for word in terms:
	matched = 1 if word in document else 0
	if matched:
	    matched_words.append(word)
	matches.append(matched)
    matches = numpy.array(matches)
    print(f"Matched Words: {matched_words}")
    print(f"Relevance: {matches.dot(query)}")
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;check_document(document_1, terms)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Document: ['the', 'movie', 'was', 'about', 'workers', 'at', 'a', 'blood', 'bank']
Matched Words: ['movie', 'blood']
Relevance: 2.0

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;check_document(document_2, terms)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Document: ['a', 'movie', 'about', 'blood', 'blood', 'and', 'more', 'blood']
Matched Words: ['movie', 'blood']
Relevance: 2.0

&lt;/pre&gt;

&lt;p&gt;
So we con see one problem with our method right off the bat, in that repeated terms don't increase the relevance so both our documents have the same score, even though one mentioned blood more often.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org76c95d6" class="outline-3"&gt;
&lt;h3 id="org76c95d6"&gt;Term Frequency Vector&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org76c95d6"&gt;
&lt;p&gt;
One approach to fix our lack of giving weight to more occurences of a keyword is to use counts, Term Frequency (TF), instead just a 0 or 1 in our vector.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def counts(document: list, terms: set) -&amp;gt; None:
    print(f"Document: {document}")
    count = Counter()
    for word in document:
	if word in terms:
	    count[word] += 1

    matches = numpy.array([count[term] for term in terms])
    print(f"Matched Words: {count}")
    print(f"Relevance: {matches.dot(query)}")
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;counts(document_1, terms)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Document: ['the', 'movie', 'was', 'about', 'workers', 'at', 'a', 'blood', 'bank']
Matched Words: Counter({'movie': 1, 'blood': 1})
Relevance: 2.0

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;counts(document_2, terms)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Document: ['a', 'movie', 'about', 'blood', 'blood', 'and', 'more', 'blood']
Matched Words: Counter({'blood': 3, 'movie': 1})
Relevance: 4.0

&lt;/pre&gt;

&lt;p&gt;
Now the extra mentions of 'blood' make it seem document 2 seem more relevant to us. What happens, though, if one of the terms is one that appears often in documents with different subjects?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;terms = "movie about blood".split()
document_3 = "This movie was not about running kites as the title implied but was rather about some kind of heart warming story meant to make the heart swoon but just about made my blood boil".split()
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;counts(document_3, terms)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Document: ['This', 'movie', 'was', 'not', 'about', 'running', 'kites', 'as', 'the', 'title', 'implied', 'but', 'was', 'rather', 'about', 'some', 'kind', 'of', 'heart', 'warming', 'story', 'meant', 'to', 'make', 'the', 'heart', 'swoon', 'but', 'just', 'about', 'made', 'my', 'blood', 'boil']
Matched Words: Counter({'about': 3, 'movie': 1, 'blood': 1})
Relevance: 5.0

&lt;/pre&gt;

&lt;p&gt;
My example is a little convoluted, but the point is that the most common words aren't necessarily the most helpful ones.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf927758" class="outline-3"&gt;
&lt;h3 id="orgf927758"&gt;Inverse Document Frequency&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf927758"&gt;
&lt;p&gt;
This method seeks to overcome the problem of words that are too common by reducing the weight of a term the more common it is.
&lt;/p&gt;

&lt;p&gt;
\[
IDF(w) = \log \left(\frac{M + 1}{k} \right)
\]
&lt;/p&gt;

&lt;p&gt;
Where &lt;i&gt;M&lt;/i&gt; is the number of documents in the collection, &lt;i&gt;w&lt;/i&gt; is the word (or term) that we are checking, and &lt;i&gt;k&lt;/i&gt; is the number of documents containing &lt;i&gt;w&lt;/i&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hover = HoverTool(
    tooltips=[
	("k", "@k"),
	("IDF(w)", "@idf")
    ],
    mode="vline",
)
M = 1000
k = numpy.linspace(1, M)
idf = numpy.log((M + 1)/k)
data = holoviews.Table((k, idf), "k", ("idf", "IDF(w)"))
plot = holoviews.Curve(data).opts(
    tools=[hover],
    height=800,
    width=1000,
    title="Inverse Document Frequency (IDF) vs Document Frequency (k)")
Embed(plot=plot, file_name="inverse_document_frequency")()
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/inverse_document_frequency.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;

&lt;p&gt;
You can see that as the number of documents with the word in it (&lt;i&gt;k&lt;/i&gt;) goes up, the weight it gets (&lt;i&gt;IDF&lt;/i&gt;) goes down until it reaches zero when all the documents have the word in it (&lt;i&gt;log(1)&lt;/i&gt; equals 0). We have codified the notion that if a word is &lt;i&gt;too&lt;/i&gt; common, then it is less important to relevance.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6daf1f8" class="outline-3"&gt;
&lt;h3 id="org6daf1f8"&gt;Ranking with TF-IDF Weighting&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6daf1f8"&gt;
&lt;p&gt;
Okay, so we hav Term-Frequency as our basic measure of relevancy, and we have this notion that the more common a word is, the less important it is to relevancy, embodied by Inverse Document Frequency (IDF), how do we use them? Like this.
&lt;/p&gt;

\begin{align}
f(q,d) &amp;amp;= \sum_{i=1}^N x_i y_i\\
       &amp;amp;= \sum_{w \in q \cap d} c(w,q) \cdot c(w, d) \log \frac{M+1}{df(w)}\\
\end{align}

&lt;p&gt;
So, let's unpack this a little.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;\(f(q, d)\) is the function to calculate the relevance of a document (&lt;i&gt;d&lt;/i&gt;) to a query (&lt;i&gt;q&lt;/i&gt;)&lt;/li&gt;
&lt;li&gt;\(w \in q \cap d\) means a word (\(w\)) in the intersection of the words in the query (\(q\)) and the words in this document (\(d\))&lt;/li&gt;
&lt;li&gt;\(c(w, q)\) is the count of the number of times this word is in the query&lt;/li&gt;
&lt;li&gt;\(c(w, d)\) is the count of the number of times this word is in the document&lt;/li&gt;
&lt;li&gt;\(M\) is the count of all documents&lt;/li&gt;
&lt;li&gt;\(df(w)\) is the number of documents with the word (document frequency of \(w\))&lt;/li&gt;
&lt;li&gt;\(\log \frac{M+1}{df(w)}\) is the &lt;i&gt;Inverse Document Frequency&lt;/i&gt; of word \(w\)&lt;/li&gt;
&lt;li&gt;\(c(w, q) \cdot c(w, d)\) is the &lt;i&gt;Term Frequency&lt;/i&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
So the relevance of a document is the sum of the products of the Term Frequency for times the Inverse Document Frequency for each word in the query. &lt;i&gt;Why is this useful?&lt;/i&gt; If a term from the query appears in the document, then it is probably more relevant than a document where it doesn't appear, and if the term appears a second time, then it reinforces the idea of its relevance, but as the term keeps appearing we get less and less assurance that it means something - does the twenty-first occurence tell us much more of its relevance than the twentieth? So we still count all the occurences (\(c(w, d)\)) but multiply it by a discounting factor to offset repetitions (the Inverse Document Frequency weight).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb61056b" class="outline-3"&gt;
&lt;h3 id="orgb61056b"&gt;Okapi BM25&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb61056b"&gt;
&lt;p&gt;
The &lt;a href="https://en.wikipedia.org/wiki/Okapi_BM25"&gt;Okapi BM25&lt;/a&gt; ranking function works in a similar way to TF-IDF, but it uses a silghtly different method. Instead of just the count of the words in a document (&lt;i&gt;c(w, d)&lt;/i&gt;), it also uses the number of documents that have the word (&lt;i&gt;k&lt;/i&gt;).
\[
y = \frac{(k + 1) c(w, d){c(w, d) + k}
\]
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hover = HoverTool(
    tooltips=[
	("c(w,d)", "@c_w_d"),
	("Y", "@y")
    ],
    mode="vline",
)
k = 20
x_limit = 1000
c_w_d = numpy.linspace(1, x_limit)
y = ((k + 1) * c_w_d)/(c_w_d + k)
data = holoviews.Table((c_w_d, y), ("c_w_d", "c(w, d)"), "y")
line = holoviews.HLine(k + 1).opts(color="red", alpha=0.4)
curve = holoviews.Curve(data).opts(
    tools=[hover],
    height=800,
    width=1000,
    title=f"BM25 c(w, d) Transformation (k={k})")
plot = (curve * line).opts(ylim=(0, k + 2))
Embed(plot=plot, file_name="bm25_transformation")()
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/bm25_transformation.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;

&lt;p&gt;
Because of the way it's set up, &lt;i&gt;k + 1&lt;/i&gt; acts as an upper bound on the output. 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org89a66c3" class="outline-3"&gt;
&lt;h3 id="org89a66c3"&gt;Document Length Normalization&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org89a66c3"&gt;
&lt;p&gt;
We have another problem to deal with - longer documents have more words and so are more likely to have the query terms, even if they aren't necessarily relevant. So one thing to do might be do drop really long documents, but some documents are long because the author was logorrheic, and others are long because the have a lot of content and so might be useful if we match them, so we don't want to throw everything away. What we want to do instead is add a penalty for longer documents.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org707be92" class="outline-4"&gt;
&lt;h4 id="org707be92"&gt;Pivoted Length Normalization&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org707be92"&gt;
&lt;p&gt;
This is a method to add a penalty to longer documents and a reward for shorter documents. Here's the equation.
&lt;/p&gt;

&lt;p&gt;
\[
normalizer = 1 -b + b \left( \frac{\textit{document length}}{\textit{average document length}} \right)
\]
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hover = HoverTool(
    tooltips=[
	("Document Length", "@document_length"),
	("Normalization", "@normalizer")
    ],
    mode="vline",
)

x_limit = 1000
average_document_length = x_limit/2
document_length = numpy.linspace(1, x_limit)
b = 0.5
normalizer = 1 - b + b * (document_length/average_document_length)
data = holoviews.Table((document_length, normalizer), 
		       ("document_length", "Document Length"), 
		       ("normalizer", 'Normalization'))
line = holoviews.VLine(average_document_length).opts(color="red", alpha=0.4)
hline = holoviews.HLine(1).opts(color="red", alpha=0.4)
curve = holoviews.Curve(data).opts(
    tools=[hover],
    height=800,
    width=1000,
    title=f"Pivoted Length Nomalization (b={b})")
plot = (curve * line * hline).opts(ylim=(0, 2))
Embed(plot=plot, file_name="pivoted_linear_transformation")()
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/pivoted_linear_transformation.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;

&lt;p&gt;
This term ends up in the denominator of the term-frequency equivalent calculations, so as the word-count goes up, the normalization grows larger, reducing the term-frequency measure, and when the word-count is low it makes the term-frequency measure larger.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3dd934e" class="outline-3"&gt;
&lt;h3 id="org3dd934e"&gt;The State of the Art&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3dd934e"&gt;
&lt;p&gt;
These are the two "best" Vector Space Model ranking functions.
&lt;/p&gt;

&lt;p&gt;
\[
b \in [0, 1]\\
k \in [0, +\inf)
\]
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org643919c" class="outline-4"&gt;
&lt;h4 id="org643919c"&gt;Pivoted Length Normalization&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org643919c"&gt;
&lt;p&gt;
\[
f(q, d) = \sum_{w \in q \cap d} c(w, q) \frac{\ln(1 + ln(1 + c(w, d)))}{1 -b + b\frac{\vert d \vert}{\textit{avg dl}}} \og \frac{M + 1}{df(w)}
\]
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9a2b7d5" class="outline-4"&gt;
&lt;h4 id="org9a2b7d5"&gt;BM25 With Document Length Normalization&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org9a2b7d5"&gt;
&lt;p&gt;
\[
f(q, d) = \sum_{w \in q \cap d} c(w, d) \frac{(k+1) c(w, d)}{c(w, d) + k(1 - b + b \frac{\vert d \vert}{\textit{avg dl}})} \log \frac{M+1}{df(w)}
\]
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5ef4ccd" class="outline-2"&gt;
&lt;h2 id="org5ef4ccd"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5ef4ccd"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb0efdd2" class="outline-3"&gt;
&lt;h3 id="orgb0efdd2"&gt;VSM Improvements&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb0efdd2"&gt;
&lt;p&gt;
These are variations that you can try to improve the effectiveness of Vector Space Model Ranking.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6bec97b" class="outline-4"&gt;
&lt;h4 id="org6bec97b"&gt;Fix the "Dimension"&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org6bec97b"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Use stemmed words, remove stop-words, use ngrams…&lt;/li&gt;
&lt;li&gt;Language and domain-specific tokenization&lt;/li&gt;
&lt;li&gt;&lt;b&gt;&lt;b&gt;In practice&lt;/b&gt;&lt;/b&gt;: Using &lt;i&gt;Bag-of-Words&lt;/i&gt; with phrases is often the best&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org216597c" class="outline-4"&gt;
&lt;h4 id="org216597c"&gt;Changing the Similarity Function&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org216597c"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Cosine Similarity&lt;/li&gt;
&lt;li&gt;Euclidean distance&lt;/li&gt;
&lt;li&gt;&lt;b&gt;&lt;b&gt;In Practice&lt;/b&gt;&lt;/b&gt;: the &lt;i&gt;Dot Product&lt;/i&gt; is still the best function&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge7cd216" class="outline-4"&gt;
&lt;h4 id="orge7cd216"&gt;Use BM25 Variations&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge7cd216"&gt;
&lt;/div&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="orgc21ccd2"&gt;&lt;/a&gt;BM25F&lt;br&gt;
&lt;div class="outline-text-5" id="text-orgc21ccd2"&gt;
&lt;p&gt;
This is for structured documents (the &lt;i&gt;F&lt;/i&gt; stands for "fields" within the document (e.g. title)).
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Uses the "fields", not just the main text&lt;/li&gt;
&lt;li&gt;Combines the counts from all the fields before applying BM25&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="orgd6c094f"&gt;&lt;/a&gt;BM25+&lt;br&gt;
&lt;div class="outline-text-5" id="text-orgd6c094f"&gt;
&lt;p&gt;
This avoids over-penalizing long documents by adding a small constant to the term-frequencies. This has been shown (and proven analytically) to be better than BM25 alone.
&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>lecture</category><category>nlp</category><category>search</category><category>text</category><guid>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/</guid><pubDate>Mon, 06 May 2019 00:13:29 GMT</pubDate></item><item><title>Natural Language Processing Review</title><link>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#org84661bb"&gt;Departure&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#org73b470d"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#org42ae74b"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#org3a3a2d8"&gt;Initiation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#org7d9ec8f"&gt;Some Vocabulary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#org5eedc76"&gt;Return&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#org394b493"&gt;Sources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org84661bb" class="outline-2"&gt;
&lt;h2 id="org84661bb"&gt;Departure&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org84661bb"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org73b470d" class="outline-3"&gt;
&lt;h3 id="org73b470d"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org73b470d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from pathlib import Path
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from spacy import displacy
import spacy
from nltk.tokenize import TweetTokenizer
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org42ae74b" class="outline-3"&gt;
&lt;h3 id="org42ae74b"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org42ae74b"&gt;
&lt;p&gt;
I'm using the &lt;code&gt;en&lt;/code&gt; model. Even after you install spacy you have to tell it which model to download.
&lt;/p&gt;

&lt;pre class="example"&gt;
python -m spacy download en
&lt;/pre&gt;

&lt;p&gt;
Once you run that this next block should work - otherwise you'll get an &lt;code&gt;OSError&lt;/code&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;processor = spacy.load("en")
tokenizer = TweetTokenizer()
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SLUG = "natural-language-processing-review"
OUTPUT_FOLDER = Path("../../files/posts/nlp/" + SLUG)
if not OUTPUT_FOLDER.is_dir():
    OUTPUT_FOLDER.mkdir(parents=True)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3a3a2d8" class="outline-2"&gt;
&lt;h2 id="org3a3a2d8"&gt;Initiation&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3a3a2d8"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7d9ec8f" class="outline-3"&gt;
&lt;h3 id="org7d9ec8f"&gt;Some Vocabulary&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7d9ec8f"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1418e2d" class="outline-4"&gt;
&lt;h4 id="org1418e2d"&gt;Natural Language Processing&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1418e2d"&gt;
&lt;p&gt;
&lt;a href="https://www.wikiwand.com/en/Natural_language_processing"&gt;Natural Language Processing&lt;/a&gt; is the computational study of human language with the aim of solving practical problems involving language. There is a related field called &lt;a href="https://www.wikiwand.com/en/Computational_linguistics"&gt;Computational Linguistics&lt;/a&gt; which is concerned more with the modeling of human language but seems related.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1f31c43" class="outline-4"&gt;
&lt;h4 id="org1f31c43"&gt;Corpora&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1f31c43"&gt;
&lt;p&gt;
A text-dataset is called a &lt;i&gt;corpus&lt;/i&gt;, usually made up of texts and metadata associated with each text. Texts are made up of characters grouped into units called &lt;i&gt;tokens&lt;/i&gt;. In English a token is generally a word or number.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf1e23f4" class="outline-5"&gt;
&lt;h5 id="orgf1e23f4"&gt;Tokenization&lt;/h5&gt;
&lt;div class="outline-text-5" id="text-orgf1e23f4"&gt;
&lt;p&gt;
The process of breaking a text up into &lt;i&gt;tokens&lt;/i&gt; is called tokenization. Here's some examples of how to do it with &lt;a href="https://spacy.io/usage/spacy-101"&gt;spacy&lt;/a&gt; and the &lt;a href="https://www.nltk.org"&gt;Natural Language Toolkit (NLTK)&lt;/a&gt;.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;text = "Those are my principles, and if you don't like them... well, I have others.".lower()
print([str(token) for token in processor(text)])
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
['those', 'are', 'my', 'principles', ',', 'and', 'if', 'you', 'do', "n't", 'like', 'them', '...', 'well', ',', 'i', 'have', 'others', '.']

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(tokenizer.tokenize(text))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
['those', 'are', 'my', 'principles', ',', 'and', 'if', 'you', "don't", 'like', 'them', '...', 'well', ',', 'i', 'have', 'others', '.']

&lt;/pre&gt;

&lt;p&gt;
Spacy prefers to break contractions up, while NLTK doesn't, otherwise they treated them pretty much the same way.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;tweet = text + " #GrouchoSaid@morning:-J".lower()
print([str(token) for token in processor(tweet)])
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
['those', 'are', 'my', 'principles', ',', 'and', 'if', 'you', 'do', "n't", 'like', 'them', '...', 'well', ',', 'i', 'have', 'others', '.', '#', 'grouchosaid@morning:-j']

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(tokenizer.tokenize(tweet))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
['those', 'are', 'my', 'principles', ',', 'and', 'if', 'you', "don't", 'like', 'them', '...', 'well', ',', 'i', 'have', 'others', '.', '#grouchosaid', '@morning', ':', '-', 'j']

&lt;/pre&gt;

&lt;p&gt;
In this case, the TweetTokenizer and spacy treated the hash-tag and smiley differently.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1f1bfef" class="outline-4"&gt;
&lt;h4 id="org1f1bfef"&gt;Types&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1f1bfef"&gt;
&lt;p&gt;
&lt;i&gt;Types&lt;/i&gt; are the unique tokens in a corpus. The set of all the types in the corpus is its &lt;i&gt;vocabulary&lt;/i&gt; or &lt;i&gt;lexicon&lt;/i&gt;. 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org13b0655" class="outline-4"&gt;
&lt;h4 id="org13b0655"&gt;Word Classes&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org13b0655"&gt;
&lt;p&gt;
There are two classes of words &lt;i&gt;content words&lt;/i&gt; and &lt;i&gt;stopwords&lt;/i&gt;. Stopwords are there mostly to glue the content words together ("a", "an", "the", etc.) and provide more noise than information.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfc21ad3" class="outline-4"&gt;
&lt;h4 id="orgfc21ad3"&gt;N-Grams&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgfc21ad3"&gt;
&lt;p&gt;
&lt;i&gt;N-grams&lt;/i&gt; are consecutive token-sequences of a fixed length (&lt;i&gt;n&lt;/i&gt;). Common special cases are:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;unigrams: &lt;i&gt;n&lt;/i&gt; = 1&lt;/li&gt;
&lt;li&gt;bigrams: &lt;i&gt;n&lt;/i&gt; = 2&lt;/li&gt;
&lt;li&gt;trigrams: &lt;i&gt;n&lt;/i&gt; = 3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
Although n-grams are generally words in some cases they can be characters - if the suffixes are meaningful, for instance.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8721d6d" class="outline-4"&gt;
&lt;h4 id="org8721d6d"&gt;Lemma&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org8721d6d"&gt;
&lt;p&gt;
&lt;a href="https://www.wikiwand.com/en/Lemma_(morphology)"&gt;Lemmas&lt;/a&gt; are root-forms for words that can have different forms. As an example - &lt;i&gt;go&lt;/i&gt; is the lemma for &lt;i&gt;go&lt;/i&gt;, &lt;i&gt;going&lt;/i&gt;, &lt;i&gt;went&lt;/i&gt;, and &lt;i&gt;gone&lt;/i&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;document = processor("an undefined problem has an infinite number of solutions")
for token in document:
    print("Token: {} -&amp;gt; Lemma: {}".format(token, token.lemma_))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Token: an -&amp;gt; Lemma: an
Token: undefined -&amp;gt; Lemma: undefined
Token: problem -&amp;gt; Lemma: problem
Token: has -&amp;gt; Lemma: have
Token: an -&amp;gt; Lemma: an
Token: infinite -&amp;gt; Lemma: infinite
Token: number -&amp;gt; Lemma: number
Token: of -&amp;gt; Lemma: of
Token: solutions -&amp;gt; Lemma: solution

&lt;/pre&gt;

&lt;p&gt;
There is a related method called &lt;i&gt;stemming&lt;/i&gt; which strips the endings off of words instead of changing to their lemmas. lemmatization is probably preferable, but is a more difficult method compared to stemming.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7481552" class="outline-4"&gt;
&lt;h4 id="org7481552"&gt;Chunking&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org7481552"&gt;
&lt;p&gt;
There are different ways of chunking text instead of just using &lt;i&gt;n-grams&lt;/i&gt;, one way is called &lt;i&gt;chunking&lt;/i&gt; which breaks the text up into phrases.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;document = processor("Mary had a little lamb, its fleece was white as snow.")
for chunk in document.noun_chunks:
    print("{} - {}".format(chunk, chunk.label_))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Mary - NP
a little lamb - NP
its fleece - NP
snow - NP

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div id="outline-container-org5eedc76" class="outline-2"&gt;
&lt;h2 id="org5eedc76"&gt;Return&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5eedc76"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org394b493" class="outline-3"&gt;
&lt;h3 id="org394b493"&gt;Sources&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org394b493"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Rao D, McMahan B. Natural language processing with PyTorch: build intelligent language applications using deep learning. Sebastopol, CA: OReilly Media; 2019.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>nlp</category><category>review</category><guid>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/</guid><pubDate>Mon, 08 Apr 2019 00:12:29 GMT</pubDate></item></channel></rss>