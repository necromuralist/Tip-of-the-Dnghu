<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tip of the Dnghu (Posts about review)</title><link>https://necromuralist.github.io/Tip-of-the-Dnghu/</link><description></description><atom:link href="https://necromuralist.github.io/Tip-of-the-Dnghu/categories/review.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2019 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Mon, 08 Apr 2019 17:13:57 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Natural Language Processing Review</title><link>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#org6b5f17f"&gt;Departure&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#orgb082093"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#org8d877d4"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#org0c5f329"&gt;Initiation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#org8c30572"&gt;Some Vocabulary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#orga0e69c4"&gt;Return&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#orgaa39cb8"&gt;Sources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6b5f17f" class="outline-2"&gt;
&lt;h2 id="org6b5f17f"&gt;Departure&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org6b5f17f"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb082093" class="outline-3"&gt;
&lt;h3 id="orgb082093"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb082093"&gt;
/home/athena/.virtualenvs/Tip-of-the-Dnghu/bin/python3: No module named virtualfish
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;import spacy
from nltk.tokenize import TweetTokenizer
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8d877d4" class="outline-3"&gt;
&lt;h3 id="org8d877d4"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8d877d4"&gt;
&lt;p&gt;
I'm using the &lt;code&gt;en&lt;/code&gt; model. Even after you install spacy you have to tell it which model to download.
&lt;/p&gt;

/home/athena/.virtualenvs/Tip-of-the-Dnghu/bin/python3: No module named virtualfish
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;python -m spacy download en
&lt;/pre&gt;&lt;/div&gt;

/home/athena/.virtualenvs/Tip-of-the-Dnghu/bin/python3: No module named virtualfish
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;processor = spacy.load("en")
tokenizer = TweetTokenizer()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0c5f329" class="outline-2"&gt;
&lt;h2 id="org0c5f329"&gt;Initiation&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0c5f329"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8c30572" class="outline-3"&gt;
&lt;h3 id="org8c30572"&gt;Some Vocabulary&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8c30572"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1b7c741" class="outline-4"&gt;
&lt;h4 id="org1b7c741"&gt;Natural Language Processing&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1b7c741"&gt;
&lt;p&gt;
&lt;a href="https://www.wikiwand.com/en/Natural_language_processing"&gt;Natural Language Processing&lt;/a&gt; is the computational study of human language with the aim of solving practical problems involving language. There is a related field called &lt;a href="https://www.wikiwand.com/en/Computational_linguistics"&gt;Computational Linguistics&lt;/a&gt; which is concerned more with the modeling of human language but seems related.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1db1f14" class="outline-4"&gt;
&lt;h4 id="org1db1f14"&gt;Corpora&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1db1f14"&gt;
&lt;p&gt;
A text-dataset is called a &lt;i&gt;corpus&lt;/i&gt;, usually made up of texts and metadata associated with each text. Texts are made up of characters grouped into units called &lt;i&gt;tokens&lt;/i&gt;. In English a token is generally a word or number.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org73e2a8a" class="outline-5"&gt;
&lt;h5 id="org73e2a8a"&gt;Tokenization&lt;/h5&gt;
&lt;div class="outline-text-5" id="text-org73e2a8a"&gt;
&lt;p&gt;
The process of breaking a text up into &lt;i&gt;tokens&lt;/i&gt; is called tokenization. Here's some examples of how to do it with &lt;a href="https://spacy.io/usage/spacy-101"&gt;spacy&lt;/a&gt; and the &lt;a href="https://www.nltk.org"&gt;Natural Language Toolkit (NLTK)&lt;/a&gt;.
&lt;/p&gt;
/home/athena/.virtualenvs/Tip-of-the-Dnghu/bin/python3: No module named virtualfish
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;text = "Those are my principles, and if you don't like them... well, I have others.".lower()
print([str(token) for token in processor(text)])
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
['those', 'are', 'my', 'principles', ',', 'and', 'if', 'you', 'do', "n't", 'like', 'them', '...', 'well', ',', 'i', 'have', 'others', '.']

&lt;/pre&gt;

/home/athena/.virtualenvs/Tip-of-the-Dnghu/bin/python3: No module named virtualfish
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(tokenizer.tokenize(text))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
['those', 'are', 'my', 'principles', ',', 'and', 'if', 'you', "don't", 'like', 'them', '...', 'well', ',', 'i', 'have', 'others', '.']

&lt;/pre&gt;

&lt;p&gt;
Spacy prefers to break contractions up, while NLTK doesn't, otherwise they treated them pretty much the same way.
&lt;/p&gt;

/home/athena/.virtualenvs/Tip-of-the-Dnghu/bin/python3: No module named virtualfish
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;tweet = text + " #GrouchoSaid@morning:-J".lower()
print([str(token) for token in processor(tweet)])
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
['those', 'are', 'my', 'principles', ',', 'and', 'if', 'you', 'do', "n't", 'like', 'them', '...', 'well', ',', 'i', 'have', 'others', '.', '#', 'grouchosaid@morning:-j']

&lt;/pre&gt;

/home/athena/.virtualenvs/Tip-of-the-Dnghu/bin/python3: No module named virtualfish
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(tokenizer.tokenize(tweet))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
['those', 'are', 'my', 'principles', ',', 'and', 'if', 'you', "don't", 'like', 'them', '...', 'well', ',', 'i', 'have', 'others', '.', '#grouchosaid', '@morning', ':', '-', 'j']

&lt;/pre&gt;

&lt;p&gt;
In this case, the TweetTokenizer and spacy treated the hash-tag and smiley differently.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb96cff9" class="outline-4"&gt;
&lt;h4 id="orgb96cff9"&gt;Types&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgb96cff9"&gt;
&lt;p&gt;
&lt;i&gt;Types&lt;/i&gt; are the unique tokens in a corpus. The set of all the types in the corpus is its &lt;i&gt;vocabulary&lt;/i&gt; or &lt;i&gt;lexicon&lt;/i&gt;. 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org48ab853" class="outline-4"&gt;
&lt;h4 id="org48ab853"&gt;Word Classes&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org48ab853"&gt;
&lt;p&gt;
There are two classes of words &lt;i&gt;content words&lt;/i&gt; and &lt;i&gt;stopwords&lt;/i&gt;. Stopwords are there mostly to glue the content words together ("a", "an", "the", etc.) and provide more noise than information.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf4f43b4" class="outline-4"&gt;
&lt;h4 id="orgf4f43b4"&gt;N-Grams&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgf4f43b4"&gt;
&lt;p&gt;
&lt;i&gt;N-grams&lt;/i&gt; are consecutive token-sequences of a fixed length (&lt;i&gt;n&lt;/i&gt;). Common special cases are:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;unigrams: &lt;i&gt;n&lt;/i&gt; = 1&lt;/li&gt;
&lt;li&gt;bigrams: &lt;i&gt;n&lt;/i&gt; = 2&lt;/li&gt;
&lt;li&gt;trigrams: &lt;i&gt;n&lt;/i&gt; = 3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
Although n-grams are generally words in some cases they can be characters - if the suffixes are meaningful, for instance.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga07b40a" class="outline-4"&gt;
&lt;h4 id="orga07b40a"&gt;Lemma&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga07b40a"&gt;
&lt;p&gt;
&lt;a href="https://www.wikiwand.com/en/Lemma_(morphology)"&gt;Lemmas&lt;/a&gt; are root-forms for words that can have different forms. As an example - &lt;i&gt;go&lt;/i&gt; is the lemma for &lt;i&gt;go&lt;/i&gt;, &lt;i&gt;going&lt;/i&gt;, &lt;i&gt;went&lt;/i&gt;, and &lt;i&gt;gone&lt;/i&gt;.
&lt;/p&gt;

/home/athena/.virtualenvs/Tip-of-the-Dnghu/bin/python3: No module named virtualfish
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;document = processor("an undefined problem has an infinite number of solutions")
for token in document:
    print("Token: {} -&amp;gt; Lemma: {}".format(token, token.lemma_))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Token: an -&amp;gt; Lemma: an
Token: undefined -&amp;gt; Lemma: undefined
Token: problem -&amp;gt; Lemma: problem
Token: has -&amp;gt; Lemma: have
Token: an -&amp;gt; Lemma: an
Token: infinite -&amp;gt; Lemma: infinite
Token: number -&amp;gt; Lemma: number
Token: of -&amp;gt; Lemma: of
Token: solutions -&amp;gt; Lemma: solution

&lt;/pre&gt;

&lt;p&gt;
There is a related method called &lt;i&gt;stemming&lt;/i&gt; which strips the endings off of words instead of changing to their lemmas. lemmatization is probably preferable, but is a more difficult method compared to stemming.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7513f9f" class="outline-4"&gt;
&lt;h4 id="org7513f9f"&gt;Chunking&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga0e69c4" class="outline-2"&gt;
&lt;h2 id="orga0e69c4"&gt;Return&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga0e69c4"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgaa39cb8" class="outline-3"&gt;
&lt;h3 id="orgaa39cb8"&gt;Sources&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgaa39cb8"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Rao D, McMahan B. Natural language processing with PyTorch: build intelligent language applications using deep learning. Sebastopol, CA: OReilly Media; 2019.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>nlp</category><category>review</category><guid>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/</guid><pubDate>Mon, 08 Apr 2019 00:12:29 GMT</pubDate></item></channel></rss>