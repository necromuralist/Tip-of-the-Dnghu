#+BEGIN_COMMENT
.. title: Word Frequencies and Zipfs Law
.. slug: word-frequencies-and-zipfs-law
.. date: 2019-06-19 17:47:38 UTC-07:00
.. tags: nlp,text-mining
.. category: Text-Mining
.. link: 
.. description: A look at word frequencies and Zipfs law.
.. type: text
.. status: 
.. updated: 

#+END_COMMENT
#+BEGIN_SRC python :session zipfs :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
** Imports
*** Python
#+begin_src python :session zipfs :results none
from argparse import Namespace
from collections import Counter
from functools import partial
from pathlib import Path
#+end_src
*** PyPi
#+begin_src python :session zipfs :results none
import holoviews
import pandas
import spacy
#+end_src
*** My Stuff
#+begin_src python :session zipfs :results none
from graeae import EnvironmentLoader, TextDownloader
from graeae.timers import Timer
from graeae.visualization import EmbedHoloview
#+end_src
** Set Up
*** Spacy
#+begin_src python :session zipfs :results none
nlp = spacy.load("en_core_web_lg")
#+end_src
*** The Path
#+begin_src python :session zipfs :results none
URL = "https://www.gutenberg.org/files/974/974-0.txt"
environment = EnvironmentLoader()
book_path = environment["GUTENBERG"]
#+end_src
*** The Timer
#+begin_src python :session zipfs :results none
TIMER = Timer()
#+end_src
** Load the File
#+BEGIN_SRC python :session zipfs :results output :exports both
book_path = Path(book_path).expanduser()
downloader = TextDownloader(url=URL, target=book_path/"conrad_joseph_secret_agent.txt")
text = downloader.download.lower()
#+END_SRC

#+RESULTS:
: 2019-06-21 13:42:40,943 [1mTextDownloader[0m download: /home/brunhilde/data/datasets/gutenberg/conrad_joseph_secret_agent.txt exists, opening it

#+BEGIN_SRC python :session zipfs :results output :exports both
print(text[:800])
#+END_SRC

#+RESULTS:
#+begin_example
                                   the
                               secret agent
                              a simple tale


                                    by
                              joseph conrad

                              second edition

                              methuen & co.,
                           36 essex street w c.
                                  london

                 _first published_ . . . _september_ 1907

                  _second edition_ . . . _october_ 1907

                                    to
                               h. g. wells

                   the chronicler of mr lewishamâ€™s love
                     the biographer of kipps and the
                      historian of the ages to come

                   this simple tale of the xi
#+end_example

*** The Document
#+begin_src python :session zipfs :results output :exports both
document = nlp(text)
#+end_src

#+RESULTS:

*** The Counter
#+begin_src python :session zipfs :results none
counter = Counter()
#+end_src

*** The Plotting
#+begin_src python :session zipfs :results none
SLUG = "word-frequencies-and-zipfs-law"
path = Path("../../files/posts/text_mining")/SLUG
Embed = partial(EmbedHoloview, folder_path=path)
holoviews.extension("bokeh")
Plot = Namespace(
    height=800,
    width=1000,
)
#+end_src
* Middle
#+begin_src python :session zipfs :results output :exports both
TIMER.message = "Finished counting tokens"
with TIMER:
    for token in document:
        counter[token.lemma_] += 1
#+end_src

#+RESULTS:
: 2019-06-21 13:43:13,871 graeae.timers.timer start: Started: 2019-06-21 13:43:13.871027
: 2019-06-21 13:43:14,046 graeae.timers.timer end: Ended: 2019-06-21 13:43:14.046282
: 2019-06-21 13:43:14,047 graeae.timers.timer end: Elapsed: 0:00:00.175255

#+begin_src python :session zipfs :results output raw :exports both
for token, count in counter.most_common()[:10]:
    print(f" - {token.strip()}: {count:,}")
#+end_src

#+RESULTS:
 - -PRON-: 8,601
 - : 6,431
 - the: 5,582
 - .: 5,578
 - ,: 5,299
 - : 4,163
 - of: 3,759
 - be: 2,828
 - ": 2,620
 - a: 2,470

So I forgot to get rid of whitespace, punctuation, etcetera.

#+begin_src python :session zipfs :results output raw :exports both
counter = Counter()
unwanted = ("PUNCT", "SPACE", "SYM")
for token in document:
    if not token.pos_ in unwanted:
        counter[token.lemma_] += 1
        
for token, count in counter.most_common()[:10]:
    print(f" - {token}: {count:,}")        
#+end_src

#+RESULTS:
 - -PRON-: 8,601
 - the: 5,582
 - of: 3,759
 - be: 2,828
 - a: 2,470
 - to: 2,106
 - and: 1,988
 - in: 1,640
 - have: 1,421
 - that: 1,056

Surprisingly, pronouns are more common than stop words.

#+begin_src python :session zipfs :results output raw :exports both
data = pandas.DataFrame(counter.most_common(), columns=["Lemma", "Count"])
curve = holoviews.Curve(data, "Lemma", "Count").opts(
    height=Plot.height,
    width=Plot.width,
    xaxis="bare",
).opts(title="Lemma Count with Whitespace and Symbols Removed")
embed = Embed(plot=curve, file_name="all_words_count")
embed()
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="all_words_count.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

#+begin_src python :session zipfs :results output raw :exports both
curve = holoviews.Curve(data, "Lemma", "Count").opts(
    height=Plot.height,
    width=Plot.width,
    xaxis="bare",
    logy=True,
    logx=True,
).opts(title="Lemma Count With Whitespace and Symbols Removed (log-log)")
embed = Embed(plot=curve, file_name="all_words_count_log")
embed()
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="all_words_count_log.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export
*** Removing Stop Words
#+begin_src python :session zipfs :results output raw :exports both
counter = Counter()
for token in document:
    if not token.is_stop and not token.pos_ in unwanted:
        counter[token.lemma_] += 1

for token, count in counter.most_common()[:10]:
    print(f" - {token}: {count}")
#+end_src

#+RESULTS:
 - verloc: 762
 - mr: 537
 - â€™s: 370
 - not: 328
 - man: 324
 - mrs: 281
 - â€™: 281
 - say: 245
 - look: 215
 - like: 213

I guess pronouns are stop words... It looks like "Verloc" is now the most common token. I assume he's the main character in the book. According to the [[https://en.wikipedia.org/wiki/The_Secret_Agent?oldformat=true][WikiPedia article about The Secret Agent]] - 

#+begin_quote
The story is set in London in 1886 and deals with Mr Anton Verloc and his work as a spy for an unnamed country (presumably Russia).
#+end_quote

#+begin_src python :session zipfs :results output raw :exports both
data = pandas.DataFrame(counter.most_common(), columns=["Lemma", "Count"])
curve = holoviews.Curve(data, "Lemma", "Count").opts(
    height=Plot.height,
    width=Plot.width,
    xaxis="bare",
    logy=True,
    logx=True,
    tools=["hover"],
).opts(title="Lemma-Count With Stopwords Removed (log-log)")
embed = Embed(plot=curve, file_name="cleaned_count")
embed()
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="cleaned_count.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

The idea behind this was to see if the word-count =The Secret Agent= obeys [[https://en.wikipedia.org/wiki/Zipf%27s_law?oldformat=true][Zipf's Law]], as it's supposed to. If it did, then the line should have straightened out once I used a log-log plot, but it doesn't appear that it did.
** Un-Lemmatized
   Maybe it's the lemmatization of the words that is messing it up.
#+begin_src python :session zipfs :results output raw :exports both
counter = Counter()
for token in document:
    if not token.pos_ in unwanted:
        counter[token.text] += 1

for token, count in counter.most_common()[:10]:
    print(f" - {token}: {count}")
#+end_src

#+RESULTS:
 - the: 5582
 - of: 3759
 - a: 2470
 - to: 2106
 - and: 1988
 - he: 1747
 - his: 1652
 - in: 1640
 - was: 1247
 - that: 1056

#+begin_src python :session zipfs :results output raw :exports both
data = pandas.DataFrame(counter.most_common(), columns=["Text", "Count"])
curve = holoviews.Curve(data, "Text", "Count").opts(
    height=Plot.height,
    width=Plot.width,
    xaxis="bare",
    logy=True,
    logx=True,
    tools=["hover"],
).opts(title="Token Frequency Counts (log-log)")
embed = Embed(plot=curve, file_name="token_count")
embed()
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="token_count.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

It doesn't appear to have helped.
*** Tokens Without Stopwords
    Does removing the stop-words help in this case?

#+begin_src python :session zipfs :results output raw :exports both
counter = Counter()
for token in document:
    if not token.pos_ in unwanted and not token.is_stop:
        counter[token.text] += 1

for token, count in counter.most_common()[:10]:
    print(f" - {token}: {count}")
#+end_src

#+RESULTS:
 - verloc: 762
 - â€™s: 623
 - mr: 537
 - nâ€™t: 328
 - mrs: 281
 - man: 260
 - said: 221
 - like: 202
 - chief: 175
 - ossipon: 175

#+begin_src python :session zipfs :results output raw :exports both
data = pandas.DataFrame(counter.most_common(), columns=["Text", "Count"])
curve = holoviews.Curve(data, "Text", "Count").opts(
    height=Plot.height,
    width=Plot.width,
    xaxis="bare",
    logy=True,
    logx=True,
    tools=["hover"],
).opts(title="Token Frequency Counts Without Stopwords (log-log)")
embed = Embed(plot=curve, file_name="token_count_no_stop")
embed()
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="token_count_no_stop.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

* End
The idea behind this was to show that word counts follow [[https://en.wikipedia.org/wiki/Zipf%27s_law?oldformat=true][Zipf's Law]], which I wasn't able to do. If it was following Zipf's law then we would expect the log-log plots to turn into straight lines.
