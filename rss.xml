<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tip of the Dnghu</title><link>https://necromuralist.github.io/Tip-of-the-Dnghu/</link><description>Natural Language Processing Notes</description><atom:link href="https://necromuralist.github.io/Tip-of-the-Dnghu/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2019 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Fri, 21 Jun 2019 19:25:18 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Word Frequencies and Zipfs Law</title><link>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/word-frequencies-and-zipfs-law/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-orga53a2e5" class="outline-2"&gt;
&lt;h2 id="orga53a2e5"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga53a2e5"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org96457d0" class="outline-3"&gt;
&lt;h3 id="org96457d0"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org96457d0"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5b6e335" class="outline-4"&gt;
&lt;h4 id="org5b6e335"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org5b6e335"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;argparse&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Namespace&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;functools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;partial&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbf7312d" class="outline-4"&gt;
&lt;h4 id="orgbf7312d"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgbf7312d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;holoviews&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;spacy&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5d6f95c" class="outline-4"&gt;
&lt;h4 id="org5d6f95c"&gt;My Stuff&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org5d6f95c"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;graeae&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;EnvironmentLoader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;TextDownloader&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;graeae.timers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Timer&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;graeae.visualization&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;EmbedHoloview&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org09823f2" class="outline-3"&gt;
&lt;h3 id="org09823f2"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org09823f2"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfc317a6" class="outline-4"&gt;
&lt;h4 id="orgfc317a6"&gt;Spacy&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgfc317a6"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;nlp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spacy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"en_core_web_lg"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge3db823" class="outline-4"&gt;
&lt;h4 id="orge3db823"&gt;The Path&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge3db823"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;URL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"https://www.gutenberg.org/files/974/974-0.txt"&lt;/span&gt;
&lt;span class="n"&gt;environment&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;EnvironmentLoader&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;book_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"GUTENBERG"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd090f6c" class="outline-4"&gt;
&lt;h4 id="orgd090f6c"&gt;The Timer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgd090f6c"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;TIMER&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Timer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6daadd8" class="outline-3"&gt;
&lt;h3 id="org6daadd8"&gt;Load the File&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6daadd8"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;book_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;book_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expanduser&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;downloader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TextDownloader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;URL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;book_path&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;"conrad_joseph_secret_agent.txt"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;downloader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-06-20 18:14:52,437 [1mTextDownloader[0m download: /home/brunhilde/data/datasets/gutenberg/conrad_joseph_secret_agent.txt exists, opening it

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
                  THE
              SECRET AGENT
             A SIMPLE TALE


                   BY
             JOSEPH CONRAD

             SECOND EDITION

             METHUEN &amp;amp; CO.,
          36 ESSEX STREET W C.
                 LONDON

_First Published_ . . . _September_ 1907

 _Second Edition_ . . . _October_ 1907

                   TO
              H. G. WELLS

  THE CHRONICLER OF MR LEWISHAM’S LOVE
    THE BIOGRAPHER OF KIPPS AND THE
     HISTORIAN OF THE AGES TO COME

  THIS SIMPLE TALE OF THE XI
&lt;/pre&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc820605" class="outline-4"&gt;
&lt;h4 id="orgc820605"&gt;The Document&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgc820605"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf540cf9" class="outline-4"&gt;
&lt;h4 id="orgf540cf9"&gt;The Counter&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgf540cf9"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;counter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org39c05c2" class="outline-4"&gt;
&lt;h4 id="org39c05c2"&gt;The Plotting&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org39c05c2"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;SLUG&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"word-frequencies-and-zipfs-law"&lt;/span&gt;
&lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"../../files/posts/text_mining"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;SLUG&lt;/span&gt;
&lt;span class="n"&gt;Embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EmbedHoloview&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;folder_path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;holoviews&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extension&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"bokeh"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Plot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Namespace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge3e12e3" class="outline-2"&gt;
&lt;h2 id="orge3e12e3"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge3e12e3"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;TIMER&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;message&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"Finished counting tokens"&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;TIMER&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;document&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="n"&gt;counter&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lemma_&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-06-20 17:41:59,573 graeae.timers.timer start: Started: 2019-06-20 17:41:59.573158
2019-06-20 17:41:59,774 graeae.timers.timer end: Ended: 2019-06-20 17:41:59.774213
2019-06-20 17:41:59,775 graeae.timers.timer end: Elapsed: 0:00:00.201055

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;counter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;most_common&lt;/span&gt;&lt;span class="p"&gt;()[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;" - {token.strip()}: {count:,}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;-PRON-: 9,376&lt;/li&gt;
&lt;li&gt;: 6,649&lt;/li&gt;
&lt;li&gt;the: 5,755&lt;/li&gt;
&lt;li&gt;.: 5,689&lt;/li&gt;
&lt;li&gt;,: 5,440&lt;/li&gt;
&lt;li&gt;: 4,240&lt;/li&gt;
&lt;li&gt;of: 3,872&lt;/li&gt;
&lt;li&gt;be: 2,887&lt;/li&gt;
&lt;li&gt;": 2,642&lt;/li&gt;
&lt;li&gt;a: 2,527&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
So I forgot to get rid of whitespace, punctuation, etcetera.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;counter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;unwanted&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"PUNCT"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"SPACE"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"SYM"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;document&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pos_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;unwanted&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="n"&gt;counter&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lemma_&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;counter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;most_common&lt;/span&gt;&lt;span class="p"&gt;()[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;" - {token}: {count:,}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;        
&lt;/pre&gt;&lt;/div&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;-PRON-: 9,376&lt;/li&gt;
&lt;li&gt;the: 5,755&lt;/li&gt;
&lt;li&gt;of: 3,872&lt;/li&gt;
&lt;li&gt;be: 2,887&lt;/li&gt;
&lt;li&gt;a: 2,526&lt;/li&gt;
&lt;li&gt;to: 2,186&lt;/li&gt;
&lt;li&gt;and: 2,055&lt;/li&gt;
&lt;li&gt;in: 1,697&lt;/li&gt;
&lt;li&gt;have: 1,419&lt;/li&gt;
&lt;li&gt;that: 1,071&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
Surprisingly, pronouns are more common than stop words.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;counter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;most_common&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Lemma"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Count"&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;curve&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;holoviews&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Curve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Lemma"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Count"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;xaxis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"bare"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Lemma"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;curve&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;file_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"all_words_count"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;embed&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/word-frequencies-and-zipfs-law/all_words_count.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;curve&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;holoviews&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Curve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Lemma"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Count"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;xaxis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"bare"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;logy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;logx&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Lemma"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;curve&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;file_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"all_words_count_log"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;embed&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/word-frequencies-and-zipfs-law/all_words_count_log.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;
&lt;/div&gt;
&lt;div id="outline-container-org960a24c" class="outline-4"&gt;
&lt;h4 id="org960a24c"&gt;Removing Stop Words&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org960a24c"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;counter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;document&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_stop&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pos_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;unwanted&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="n"&gt;counter&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lemma_&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;counter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;most_common&lt;/span&gt;&lt;span class="p"&gt;()[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;" - {token}: {count}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;Verloc: 762&lt;/li&gt;
&lt;li&gt;Mr: 536&lt;/li&gt;
&lt;li&gt;’s: 387&lt;/li&gt;
&lt;li&gt;not: 328&lt;/li&gt;
&lt;li&gt;man: 323&lt;/li&gt;
&lt;li&gt;Mrs: 281&lt;/li&gt;
&lt;li&gt;’: 266&lt;/li&gt;
&lt;li&gt;say: 245&lt;/li&gt;
&lt;li&gt;look: 215&lt;/li&gt;
&lt;li&gt;like: 213&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
I guess pronouns are stop words… It looks like "Verloc" is now the most common token. I assume he's the main character in the book. According to the &lt;a href="https://en.wikipedia.org/wiki/The_Secret_Agent?oldformat=true"&gt;WikiPedia article about The Secret Agent&lt;/a&gt; - 
&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;
The story is set in London in 1886 and deals with Mr Anton Verloc and his work as a spy for an unnamed country (presumably Russia).
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;counter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;most_common&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Lemma"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Count"&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;curve&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;holoviews&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Curve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Lemma"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Count"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;xaxis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"bare"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;logy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;logx&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;tools&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"hover"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Lemma"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;curve&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;file_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"cleaned_count"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;embed&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/word-frequencies-and-zipfs-law/cleaned_count.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org97f442c" class="outline-2"&gt;
&lt;h2 id="org97f442c"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org97f442c"&gt;
&lt;p&gt;
The idea behind this was to show that word counts follow &lt;a href="https://en.wikipedia.org/wiki/Zipf's_law?oldformat=true"&gt;Zipf's Law&lt;/a&gt;, which I wasn't able to do. If it was following Zipf's law then we would expect the log-log plots to turn into straight lines.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>nlp</category><category>text-mining</category><guid>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/word-frequencies-and-zipfs-law/</guid><pubDate>Thu, 20 Jun 2019 00:47:38 GMT</pubDate></item><item><title>Name Redaction</title><link>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/name-redaction/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/name-redaction/#org5485fbf"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/name-redaction/#orgbee770d"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/name-redaction/#orgd5666d0"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/name-redaction/#org21c410b"&gt;Load the File&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/name-redaction/#org62abcee"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/name-redaction/#org1d02e2f"&gt;A Little spacy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/name-redaction/#orge7f16da"&gt;Redacting Names&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/name-redaction/#org8dbbaee"&gt;End&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/name-redaction/#org85568b9"&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5485fbf" class="outline-2"&gt;
&lt;h2 id="org5485fbf"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5485fbf"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbee770d" class="outline-3"&gt;
&lt;h3 id="orgbee770d"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgbee770d"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org18df4e1" class="outline-4"&gt;
&lt;h4 id="org18df4e1"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org18df4e1"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from pathlib import Path
from typing import Collection
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org99afbfe" class="outline-4"&gt;
&lt;h4 id="org99afbfe"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org99afbfe"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from spacy import displacy

import spacy
import textacy
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb86812e" class="outline-4"&gt;
&lt;h4 id="orgb86812e"&gt;My Stuff&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgb86812e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from graeae.timers import Timer
from graeae import EnvironmentLoader, TextDownloader
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd5666d0" class="outline-3"&gt;
&lt;h3 id="orgd5666d0"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd5666d0"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org97e3320" class="outline-4"&gt;
&lt;h4 id="org97e3320"&gt;The Timer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org97e3320"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;TIMER = Timer()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org43d1495" class="outline-4"&gt;
&lt;h4 id="org43d1495"&gt;Spacy&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org43d1495"&gt;
&lt;p&gt;
This loads the large English model tha spacy provides.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;with TIMER:
    nlp = spacy.load("en_core_web_lg")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-05-27 16:57:09,330 graeae.timers.timer start: Started: 2019-05-27 16:57:09.330711
2019-05-27 16:57:15,562 graeae.timers.timer end: Ended: 2019-05-27 16:57:15.562439
2019-05-27 16:57:15,563 graeae.timers.timer end: Elapsed: 0:00:06.231728

&lt;/pre&gt;

&lt;p&gt;
Although it took a long time to download the model doesn't actually take a long time to load.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org21c410b" class="outline-3"&gt;
&lt;h3 id="org21c410b"&gt;Load the File&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org21c410b"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;URL = "https://www.gutenberg.org/files/974/974-0.txt"
environment = EnvironmentLoader()
path = environment["GUTENBERG"]
path = Path(path).expanduser()
downloader = TextDownloader(url=URL, target=path/"conrad_joseph_secret_agent.txt")
text = downloader.download
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-05-27 17:10:07,016 [1mTextDownloader[0m download: /home/athena/data/datasets/gutenberg/conrad_joseph_secret_agent.txt exists, opening it

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div id="outline-container-org62abcee" class="outline-2"&gt;
&lt;h2 id="org62abcee"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org62abcee"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1d02e2f" class="outline-3"&gt;
&lt;h3 id="org1d02e2f"&gt;A Little spacy&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1d02e2f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;document = nlp(text)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
We want to be able to block out all the names in documents to preserve people's anonimity. One way to do that is to use spaCy's &lt;a href="https://spacy.io/usage/linguistic-features#named-entities"&gt;Named Entity Recognition&lt;/a&gt; which assigns labels to spans of tokens that it has identified to be a type of "entity". The main type of entity we're interested in here is a &lt;code&gt;PERSON&lt;/code&gt;.
&lt;/p&gt;

&lt;p&gt;
Entities are objects that have been given names. spaCy identifies them using a statistical model, so our ability to accurately identify them is relying on our documents resembling the ones their model was trained on.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;names = set()
for entity in document[:10000].ents:
    if entity.label_=="PERSON" and not entity.text in names:
	print(f"{entity.text}")
	names.add(entity.text)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Verloc
Mrs Verloc
Winnie Verloc
Mrs
Verloc’s
Mrs Verloc’s
Winnie
Winnie

Stevie

Stevie
Verloc’s
Verlocs
Stephen
Mr Verloc
Chesham Square
Wurmt
Vladimir
Mr Vladimir
Cherchez
Stott-Wartenheim
Verloc

Bosh
Mr Verloc’s
Romuald
Vox
Milan
F. P.
this F. P.?
&lt;/pre&gt;

&lt;p&gt;
You can see that it's pretty good at getting names, although it isn't perfect. There are other entity types besides "PERSON", and there's too many entities to to look at each of them:
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(f"{len(document.ents):,}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2,481

&lt;/pre&gt;

&lt;p&gt;
But maybe we can look at the types.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;entity_types = {entity.label_ for entity in document.ents}
print(len(entity_types))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
18

&lt;/pre&gt;

&lt;p&gt;
Only eighteen of them, that's not too bad. spaCy has a built-in function named &lt;code&gt;explain&lt;/code&gt; that let's you look up a little more information about the entity types.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print("|Entity Type| Explanation|")
print("|-+-|")
for entity_type in sorted(entity_types):
    print(f"|{entity_type}| {spacy.explain(entity_type)}|")
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Entity Type&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Explanation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;CARDINAL&lt;/td&gt;
&lt;td class="org-left"&gt;Numerals that do not fall under another type&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;DATE&lt;/td&gt;
&lt;td class="org-left"&gt;Absolute or relative dates or periods&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;EVENT&lt;/td&gt;
&lt;td class="org-left"&gt;Named hurricanes, battles, wars, sports events, etc.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;FAC&lt;/td&gt;
&lt;td class="org-left"&gt;Buildings, airports, highways, bridges, etc.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;GPE&lt;/td&gt;
&lt;td class="org-left"&gt;Countries, cities, states&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;LANGUAGE&lt;/td&gt;
&lt;td class="org-left"&gt;Any named language&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;LAW&lt;/td&gt;
&lt;td class="org-left"&gt;Named documents made into laws.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;LOC&lt;/td&gt;
&lt;td class="org-left"&gt;Non-GPE locations, mountain ranges, bodies of water&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;MONEY&lt;/td&gt;
&lt;td class="org-left"&gt;Monetary values, including unit&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;NORP&lt;/td&gt;
&lt;td class="org-left"&gt;Nationalities or religious or political groups&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;ORDINAL&lt;/td&gt;
&lt;td class="org-left"&gt;"first", "second", etc.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;ORG&lt;/td&gt;
&lt;td class="org-left"&gt;Companies, agencies, institutions, etc.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;PERCENT&lt;/td&gt;
&lt;td class="org-left"&gt;Percentage, including "%"&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;PERSON&lt;/td&gt;
&lt;td class="org-left"&gt;People, including fictional&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;PRODUCT&lt;/td&gt;
&lt;td class="org-left"&gt;Objects, vehicles, foods, etc. (not services)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;QUANTITY&lt;/td&gt;
&lt;td class="org-left"&gt;Measurements, as of weight or distance&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;TIME&lt;/td&gt;
&lt;td class="org-left"&gt;Times smaller than a day&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;WORK_OF_ART&lt;/td&gt;
&lt;td class="org-left"&gt;Titles of books, songs, etc.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
The categories seem kind of arbitrary, but perhaps that's because of the book that I chose. 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge7f16da" class="outline-3"&gt;
&lt;h3 id="orge7f16da"&gt;Redacting Names&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge7f16da"&gt;
&lt;p&gt;
It's getting a little unwieldy to handle the entire &lt;i&gt;Secret Agent&lt;/i&gt; novel so I'll switch to somet oy sentence fragments.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fragment = "Mr. Jason Ottomatic and Tom Tuttle (of Tacoma) went to see Billy Buttman at Barney's."
document = nlp(fragment)
for entity in document.ents:
    print(f"{entity.text}: {entity.label_}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Jason Ottomatic: PERSON
Tom Tuttle: PERSON
Tacoma: GPE
Billy Buttman: PERSON
Barney's: ORG

&lt;/pre&gt;

&lt;p&gt;
Although this seems kind of slow, we can iterate over the entities and replace the ones that we identify as a person with a &lt;code&gt;[REDACTED]&lt;/code&gt; symbol.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def redact_names(sentence: str) -&amp;gt; str:
    """Takes a sentence and redacts people's names

    Args:
     sentence: the text to redact

    Returns:
     redacted sentence
    """
    document = nlp(sentence)
    return " ".join(("[REDACTED]" if token.ent_type_=="PERSON" else token.text 
		    for token in document))
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
How does that do?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(redact_names(fragment))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Mr. [REDACTED] [REDACTED] and [REDACTED] [REDACTED] ( of Tacoma ) went to see [REDACTED] [REDACTED] at Barney 's .

&lt;/pre&gt;

&lt;p&gt;
Well, that, surprisingly, didn't work the way I thought it would. Entities represent spans of tokens which it keeps together, but now that we're using tokens we end up with one &lt;code&gt;[REDACTED]&lt;/code&gt; for each token in their names, which isn't what we want. What if we use entities?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;document = nlp(fragment)
print(" ".join(("[REDACTED]" if entity.label_=="PERSON" else entity.text 
		     for entity in document.ents)))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
[REDACTED] [REDACTED] Tacoma [REDACTED] Barney's

&lt;/pre&gt;

&lt;p&gt;
No, because not all the tokens are entities (it cleans out things like stop-words and punctuation). The secret turns out to be to tell the entities to merge the tokens together before we pull out the tokens.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def redact_name_3(sentence : str) -&amp;gt; str:
    """Takes a sentence and redacts people's names

    Args:
     sentence: the text to redact

    Returns:
     redacted sentence
    """
    document = nlp(sentence)
    for entity in document.ents:
	entity.merge()
    return "".join(("[REDACTED] " if token.ent_type_=="PERSON" else token.string
		    for token in document))
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(redact_name_3(fragment))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Mr. [REDACTED] and [REDACTED] (of Tacoma) went to see [REDACTED] at Barney's.

&lt;/pre&gt;

&lt;p&gt;
Besides the merge I switched to using &lt;code&gt;token.string&lt;/code&gt; which (mostly) keeeps the whitespace.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orged3a213" class="outline-4"&gt;
&lt;h4 id="orged3a213"&gt;More Redaction&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orged3a213"&gt;
&lt;p&gt;
We've been told that our identification of where the second person is from, and where all three met might give out too much information as well. Looking at the list of &lt;a href="https://spacy.io/usage/linguistic-features#named-entities"&gt;bulit-in named entities&lt;/a&gt; doesn't make it obvious what the two entity types would be, so I guess I'll brute-force it.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;document = nlp(fragment)
print("|Token| Type|")
print("|-+-|")
for entity in document.ents:
    print(f"|{entity.text}| {entity.label_}")
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Token&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;Jason Ottomatic&lt;/td&gt;
&lt;td class="org-left"&gt;PERSON&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Tom Tuttle&lt;/td&gt;
&lt;td class="org-left"&gt;PERSON&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Tacoma&lt;/td&gt;
&lt;td class="org-left"&gt;GPE&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Billy Buttman&lt;/td&gt;
&lt;td class="org-left"&gt;PERSON&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Barney's&lt;/td&gt;
&lt;td class="org-left"&gt;ORG&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;exclude = ("PERSON", "GPE", "ORG")

def redact_name_4(sentence : str, exclude: Collection=exclude) -&amp;gt; str:
    """Takes a sentence and redacts people's names

    Args:
     sentence: the text to redact
     exclude: collection of entitiy types to exclude

    Returns:
     redacted sentence
    """
    document = nlp(sentence)
    for entity in document.ents:
	entity.merge()
    return "".join(("[REDACTED] " if token.ent_type_ in exclude else token.string
		    for token in document))
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(redact_name_4(fragment))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Mr. [REDACTED] and [REDACTED] (of [REDACTED] ) went to see [REDACTED] at [REDACTED] .

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fragment = "President Johnson called an emergency Congressional session to discuss the gathering clouds of war."
redacted = redact_name_4(fragment)
print(redacted)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
President [REDACTED] called an emergency [REDACTED] session to discuss the gathering clouds of war.

&lt;/pre&gt;

&lt;p&gt;
Well, the fact that "President" got through might make it a little bit identifying. If you've got the president involved, though, you probably want to be a little more careful anyway.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print("|Token | Type|")
print("|-+-|")
document = nlp(fragment)
for entity in document.ents:
    print(f"|{entity.text}| {entity.label_}|")
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Token&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;Johnson&lt;/td&gt;
&lt;td class="org-left"&gt;PERSON&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Congressional&lt;/td&gt;
&lt;td class="org-left"&gt;ORG&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
It looks like "President" isn't one of the entities spacy knows about. So perhaps in this case a regular expression might be in order.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8dbbaee" class="outline-2"&gt;
&lt;h2 id="org8dbbaee"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org8dbbaee"&gt;
&lt;p&gt;
This was a brief look at how you can use a slightly more informed approach to identify parts of a text without using regular expressions and things of that nature to match strings. By using identifiable named entities, spacy is able to help us identify entire classes of entities to match without knowing what they look like beforehand. Of course, it would be dangerous to do this too blindly, there might always be things that confuse the model, but spacy does fairly well right out of the box.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org85568b9" class="outline-3"&gt;
&lt;h3 id="org85568b9"&gt;Reference&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org85568b9"&gt;
&lt;p&gt;
This was based on a chapter in this book.
&lt;/p&gt;

&lt;ol class="org-ol"&gt;
&lt;li&gt;Kasliwal N. Natural language processing with Python quick start guide: going from a Python developer to an effective natural language processing engineer [Internet]. 2018 [cited 2019 May 18]. Available from: &lt;a href="http://proquest.safaribooksonline.com/?fpi=9781789130386"&gt;http://proquest.safaribooksonline.com/?fpi=9781789130386&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>cleaning</category><category>data</category><guid>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/name-redaction/</guid><pubDate>Mon, 27 May 2019 23:17:18 GMT</pubDate></item><item><title>Some Tools For Tidying Data</title><link>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/tidying-data/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/tidying-data/#orgf68fcc7"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/tidying-data/#org0ad31f0"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/tidying-data/#org7278fda"&gt;The Text File.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/tidying-data/#orgd4351b6"&gt;Setup Spacy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/tidying-data/#orge99aa54"&gt;The Timer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/tidying-data/#orge67f3c5"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/tidying-data/#orgae79d5d"&gt;Load the File&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/tidying-data/#org82a4d79"&gt;Reload&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/tidying-data/#org2c788b8"&gt;Tokenizing with Spacy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/tidying-data/#org7112614"&gt;A Detour Into Fuzzy Wuzzy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/tidying-data/#org1604c14"&gt;A Diversion Into JellyFish&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/tidying-data/#org9f8571f"&gt;A Short Diversion Into FlashText&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/tidying-data/#org515abb4"&gt;End&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/tidying-data/#org08a394e"&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf68fcc7" class="outline-2"&gt;
&lt;h2 id="orgf68fcc7"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf68fcc7"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0ad31f0" class="outline-3"&gt;
&lt;h3 id="org0ad31f0"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0ad31f0"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6e0de13" class="outline-4"&gt;
&lt;h4 id="org6e0de13"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org6e0de13"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;difflib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SequenceMatcher&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;typing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Callable&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org937d237" class="outline-4"&gt;
&lt;h4 id="org937d237"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org937d237"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;flashtext&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;KeywordProcessor&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;fuzzywuzzy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;fuzz&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;process&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tabulate&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;tabulate&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;spacy&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;jellyfish&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgff01f9d" class="outline-4"&gt;
&lt;h4 id="orgff01f9d"&gt;My Projects&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgff01f9d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;graeae&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;EnvironmentLoader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;TextDownloader&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;graeae.timers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Timer&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7278fda" class="outline-3"&gt;
&lt;h3 id="org7278fda"&gt;The Text File.&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7278fda"&gt;
&lt;p&gt;
This is the URL to Joseph Conrad's "The Secret Agent" from &lt;a href="https://www.gutenberg.org/ebooks/974"&gt;Project Gutenberg&lt;/a&gt;.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;URL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"https://www.gutenberg.org/files/974/974-0.txt"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;environment&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;EnvironmentLoader&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"GUTENBERG"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd4351b6" class="outline-3"&gt;
&lt;h3 id="orgd4351b6"&gt;Setup Spacy&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd4351b6"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;nlp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spacy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"en_core_web_md"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge99aa54" class="outline-3"&gt;
&lt;h3 id="orge99aa54"&gt;The Timer&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge99aa54"&gt;
&lt;p&gt;
This just tracks how longs things take.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;TIMER&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Timer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge67f3c5" class="outline-2"&gt;
&lt;h2 id="orge67f3c5"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge67f3c5"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgae79d5d" class="outline-3"&gt;
&lt;h3 id="orgae79d5d"&gt;Load the File&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgae79d5d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expanduser&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;downloader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TextDownloader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;URL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;"conrad_joseph_secret_agent.txt"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;downloader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-05-26 15:59:52,186 [1mTextDownloader[0m download: /home/athena/data/datasets/gutenberg/conrad_joseph_secret_agent.txt exists, opening it

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
                  THE
              SECRET AGENT
             A SIMPLE TALE


                   BY
             JOSEPH CONRAD

             SECOND EDITION

             METHUEN &amp;amp; CO.,
          36 ESSEX STREET W C.
                 LONDON

_First Published_ . . . _September_ 1907

 _Second Edition_ . . . _October_ 1907

                   TO
              H. G. WELLS

  THE CHRONICLER OF MR LEWISHAM’S LOVE
    THE BIOGRAPHER OF KIPPS AND THE
     HISTORIAN OF THE AGES TO COME

  THIS SIMPLE TALE OF THE XI
&lt;/pre&gt;

&lt;p&gt;
So it looks like the actual start of our book comes after the transcription credit.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;lines&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;end&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lines&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="s1"&gt;'ccx074@pglaf.org'&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"The Credit line is {index}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="n"&gt;end&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="s2"&gt;"THE"&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"The first line of our text is at {index}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="k"&gt;break&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
I'll get rid of the header.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;cleaned&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lines&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;38&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;""&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cleaned&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"{len(cleaned):,}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
his wife was in charge of his brother-in-law.
10,083

&lt;/pre&gt;

&lt;p&gt;
So, there's 10,121 lines in the book (I don't know if that is long for a book, I don't think it is) and there are carriage returns (or whatever those marks are called) in the text. I'm going to save the file with the header removed and clean the file separately with &lt;code&gt;dos2unix&lt;/code&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;downloader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'w'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cleaned&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org82a4d79" class="outline-3"&gt;
&lt;h3 id="org82a4d79"&gt;Reload&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org82a4d79"&gt;
&lt;p&gt;
I ran &lt;code&gt;dos2unix&lt;/code&gt; on the file, let's see if it's better.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;downloader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_download&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
&lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;downloader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-05-26 15:59:56,659 [1mTextDownloader[0m download: /home/athena/data/datasets/gutenberg/conrad_joseph_secret_agent.txt exists, opening it

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;700&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
his wife was in charge of his brother-in-law.

The shop was small, and so was the house.  It was one of those grimy
brick houses which existed in large quantities before the era of
reconstruction dawned upon London.  The shop was a square box of a place,
with the front glazed in small panes.  In the daytime the door remained
closed; in the evening it stood discreetly but suspiciously ajar.

The window contained photographs of more or less undressed dancing girls;
nondescript packages in wrappers like patent medicines; closed yellow
paper envelopes, very flimsy, and marked two-and-six in heavy black
figures; a few numbers of ancient French comic publications hung across a
string as if to dry;
&lt;/pre&gt;

&lt;p&gt;
How many unique characters are there?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
90

&lt;/pre&gt;

&lt;p&gt;
According to &lt;a href="https://www.wikiwand.com/en/ASCII"&gt;Wikipedia&lt;/a&gt;, there are 95 printable ASCII characters so this doesn't use all of them, but comes close.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org2c788b8" class="outline-3"&gt;
&lt;h3 id="org2c788b8"&gt;Tokenizing with Spacy&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2c788b8"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;TIMER&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;document&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nlp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-05-26 16:46:53,678 graeae.timers.timer start: Started: 2019-05-26 16:46:53.677998
2019-05-26 16:47:05,602 graeae.timers.timer end: Ended: 2019-05-26 16:47:05.602303
2019-05-26 16:47:05,602 graeae.timers.timer end: Elapsed: 0:00:11.924305

&lt;/pre&gt;

&lt;p&gt;
Spacy pre-computes the linguistic features when you create the &lt;code&gt;document&lt;/code&gt; instance so it takes a little longer to load than you might expect.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;document&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
&amp;lt;class 'spacy.tokens.doc.Doc'&amp;gt;

&lt;/pre&gt;

&lt;p&gt;
Our document is a spacy &lt;a href="https://spacy.io/api/doc"&gt;Doc&lt;/a&gt; instance which they describe as a container for accessing language annotations. They also describe it as a sequence of TokenCJ structs (whatever those are).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;document&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
&amp;lt;class 'spacy.tokens.token.Token'&amp;gt;

&lt;/pre&gt;

&lt;p&gt;
So it looks like besides having its own methods, the Doc holds &lt;a href="https://spacy.io/api/token"&gt;Token&lt;/a&gt; objects.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;document&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
dancing

&lt;/pre&gt;

&lt;p&gt;
If you grab a span of tokens instead of a single token it renders them as a &lt;a href="https://spacy.io/api/span"&gt;Span&lt;/a&gt; object.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;document&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
dancing girls;
nondescript packages in wrappers like patent medicines; closed yellow
paper envelopes, very flimsy, and marked two-and-six in heavy black
figures; a few numbers of ancient French comic publications hung across a
string as if to

&lt;/pre&gt;

&lt;p&gt;
Although the square brackets are token-based, the document also allows you to grab sentences.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sentences&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;document&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sents&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sentence&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
the attractions of stray cats and dogs, which he followed down narrow
alleys into unsavoury courts; by the comedies of the streets, which he
contemplated open-mouthed, to the detriment of
&amp;lt;class 'spacy.tokens.span.Span'&amp;gt;

&lt;/pre&gt;

&lt;p&gt;
I had to convert it to a tuple because &lt;code&gt;sents&lt;/code&gt; is actually a generator, not a collection.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgbaf50d6" class="outline-4"&gt;
&lt;h4 id="orgbaf50d6"&gt;Token Attributes&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgbaf50d6"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"|Token | Stop-Word| Punctuation |"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"|-+-+-+-|"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sentence&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_space&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="k"&gt;continue&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"|{token}| {token.is_stop}| {token.is_punct}|"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Token&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Stop-Word&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Punctuation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;the&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;attractions&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;of&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;stray&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;cats&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;and&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;dogs&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;,&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;which&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;he&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;followed&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;down&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;narrow&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;alleys&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;into&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;unsavoury&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;courts&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;;&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;by&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;the&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;comedies&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;of&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;the&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;streets&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;,&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;which&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;he&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;contemplated&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;open&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;mouthed&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;,&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;to&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;the&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;detriment&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;of&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
So spacy can help us identify different types of tokens, in this case stopwords, punctuation, and spaces, but it can do more. The stop-words it uses are kept in a dictionary that you can add to to make it more domain-specific. Here's what the sentence looks like if you filter out the stopword, punctuation, and spaces.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sentence&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="nb"&gt;any&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_stop&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		&lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_punct&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		&lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_space&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
	&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"- {token}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;attractions&lt;/li&gt;
&lt;li&gt;stray&lt;/li&gt;
&lt;li&gt;cats&lt;/li&gt;
&lt;li&gt;dogs&lt;/li&gt;
&lt;li&gt;followed&lt;/li&gt;
&lt;li&gt;narrow&lt;/li&gt;
&lt;li&gt;alleys&lt;/li&gt;
&lt;li&gt;unsavoury&lt;/li&gt;
&lt;li&gt;courts&lt;/li&gt;
&lt;li&gt;comedies&lt;/li&gt;
&lt;li&gt;streets&lt;/li&gt;
&lt;li&gt;contemplated&lt;/li&gt;
&lt;li&gt;open&lt;/li&gt;
&lt;li&gt;mouthed&lt;/li&gt;
&lt;li&gt;detriment&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org93468c2" class="outline-4"&gt;
&lt;h4 id="org93468c2"&gt;Lemmatisation&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org93468c2"&gt;
&lt;p&gt;
Spacy implements &lt;a href="https://www.wikiwand.com/en/Lemmatisation"&gt;Lemmatisation&lt;/a&gt;, the conversion of a token to the "standard form" for a word.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"|Token| Lemma|Part of Speech|"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"|-+-+-|"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sentence&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_space&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"|{token}| {token.lemma_} |{token.pos_}|"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Token&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Lemma&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Part of Speech&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;the&lt;/td&gt;
&lt;td class="org-left"&gt;the&lt;/td&gt;
&lt;td class="org-left"&gt;DET&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;attractions&lt;/td&gt;
&lt;td class="org-left"&gt;attraction&lt;/td&gt;
&lt;td class="org-left"&gt;NOUN&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;of&lt;/td&gt;
&lt;td class="org-left"&gt;of&lt;/td&gt;
&lt;td class="org-left"&gt;ADP&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;stray&lt;/td&gt;
&lt;td class="org-left"&gt;stray&lt;/td&gt;
&lt;td class="org-left"&gt;ADJ&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;cats&lt;/td&gt;
&lt;td class="org-left"&gt;cat&lt;/td&gt;
&lt;td class="org-left"&gt;NOUN&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;and&lt;/td&gt;
&lt;td class="org-left"&gt;and&lt;/td&gt;
&lt;td class="org-left"&gt;CCONJ&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;dogs&lt;/td&gt;
&lt;td class="org-left"&gt;dog&lt;/td&gt;
&lt;td class="org-left"&gt;NOUN&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;,&lt;/td&gt;
&lt;td class="org-left"&gt;,&lt;/td&gt;
&lt;td class="org-left"&gt;PUNCT&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;which&lt;/td&gt;
&lt;td class="org-left"&gt;which&lt;/td&gt;
&lt;td class="org-left"&gt;DET&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;he&lt;/td&gt;
&lt;td class="org-left"&gt;-PRON-&lt;/td&gt;
&lt;td class="org-left"&gt;PRON&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;followed&lt;/td&gt;
&lt;td class="org-left"&gt;follow&lt;/td&gt;
&lt;td class="org-left"&gt;VERB&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;down&lt;/td&gt;
&lt;td class="org-left"&gt;down&lt;/td&gt;
&lt;td class="org-left"&gt;PART&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;narrow&lt;/td&gt;
&lt;td class="org-left"&gt;narrow&lt;/td&gt;
&lt;td class="org-left"&gt;ADJ&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;alleys&lt;/td&gt;
&lt;td class="org-left"&gt;alley&lt;/td&gt;
&lt;td class="org-left"&gt;NOUN&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;into&lt;/td&gt;
&lt;td class="org-left"&gt;into&lt;/td&gt;
&lt;td class="org-left"&gt;ADP&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;unsavoury&lt;/td&gt;
&lt;td class="org-left"&gt;unsavoury&lt;/td&gt;
&lt;td class="org-left"&gt;ADJ&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;courts&lt;/td&gt;
&lt;td class="org-left"&gt;court&lt;/td&gt;
&lt;td class="org-left"&gt;NOUN&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;;&lt;/td&gt;
&lt;td class="org-left"&gt;;&lt;/td&gt;
&lt;td class="org-left"&gt;PUNCT&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;by&lt;/td&gt;
&lt;td class="org-left"&gt;by&lt;/td&gt;
&lt;td class="org-left"&gt;ADP&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;the&lt;/td&gt;
&lt;td class="org-left"&gt;the&lt;/td&gt;
&lt;td class="org-left"&gt;DET&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;comedies&lt;/td&gt;
&lt;td class="org-left"&gt;comedy&lt;/td&gt;
&lt;td class="org-left"&gt;NOUN&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;of&lt;/td&gt;
&lt;td class="org-left"&gt;of&lt;/td&gt;
&lt;td class="org-left"&gt;ADP&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;the&lt;/td&gt;
&lt;td class="org-left"&gt;the&lt;/td&gt;
&lt;td class="org-left"&gt;DET&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;streets&lt;/td&gt;
&lt;td class="org-left"&gt;street&lt;/td&gt;
&lt;td class="org-left"&gt;NOUN&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;,&lt;/td&gt;
&lt;td class="org-left"&gt;,&lt;/td&gt;
&lt;td class="org-left"&gt;PUNCT&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;which&lt;/td&gt;
&lt;td class="org-left"&gt;which&lt;/td&gt;
&lt;td class="org-left"&gt;DET&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;he&lt;/td&gt;
&lt;td class="org-left"&gt;-PRON-&lt;/td&gt;
&lt;td class="org-left"&gt;PRON&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;contemplated&lt;/td&gt;
&lt;td class="org-left"&gt;contemplate&lt;/td&gt;
&lt;td class="org-left"&gt;VERB&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;open&lt;/td&gt;
&lt;td class="org-left"&gt;open&lt;/td&gt;
&lt;td class="org-left"&gt;ADJ&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;mouthed&lt;/td&gt;
&lt;td class="org-left"&gt;mouthed&lt;/td&gt;
&lt;td class="org-left"&gt;ADJ&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;,&lt;/td&gt;
&lt;td class="org-left"&gt;,&lt;/td&gt;
&lt;td class="org-left"&gt;PUNCT&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;to&lt;/td&gt;
&lt;td class="org-left"&gt;to&lt;/td&gt;
&lt;td class="org-left"&gt;ADP&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;the&lt;/td&gt;
&lt;td class="org-left"&gt;the&lt;/td&gt;
&lt;td class="org-left"&gt;DET&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;detriment&lt;/td&gt;
&lt;td class="org-left"&gt;detriment&lt;/td&gt;
&lt;td class="org-left"&gt;NOUN&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;of&lt;/td&gt;
&lt;td class="org-left"&gt;of&lt;/td&gt;
&lt;td class="org-left"&gt;ADP&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
It doesn't look like there's a lot of conversion being done, other than reducing plural to single, but if you look at &lt;i&gt;comedies&lt;/i&gt; you can see that it was lemmatised as &lt;i&gt;comedy&lt;/i&gt;, which is a little more sophisticated than just chopping off the last letter.
&lt;/p&gt;

&lt;p&gt;
I filtered out the spaces because it broke my table, but it's part-of-speech label was &lt;code&gt;SPACE&lt;/code&gt;. The &lt;code&gt;-PRON-&lt;/code&gt; lemma is a special one that spaCy uses for any &lt;a href="https://www.wikiwand.com/en/Pronoun"&gt;pronoun&lt;/a&gt; (I, we she, etc.). According to the &lt;a href="https://spacy.io/api/annotation"&gt;spaCy annotation documentation&lt;/a&gt;, the space lemma is only included if there's more than one, which they include because multiple spaces might be significant.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7112614" class="outline-3"&gt;
&lt;h3 id="org7112614"&gt;A Detour Into Fuzzy Wuzzy&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7112614"&gt;
&lt;p&gt;
&lt;a href="https://github.com/seatgeek/fuzzywuzzy"&gt;fuzzywuzzy&lt;/a&gt; is a library that does fuzzy string matching using the &lt;a href="https://www.wikiwand.com/en/Levenshtein_distance"&gt;Levenshtein Distance&lt;/a&gt; between strings. There's a &lt;a href="https://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/"&gt;page&lt;/a&gt; showing more about how to use it based the example of finding concert information on the web.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6f1745d" class="outline-4"&gt;
&lt;h4 id="org6f1745d"&gt;Ratio&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org6f1745d"&gt;
&lt;p&gt;
The &lt;code&gt;ratio&lt;/code&gt; function for fuzzywuzzy is an alias for the &lt;a href="https://docs.python.org/3/library/difflib.html"&gt;difflib&lt;/a&gt; &lt;code&gt;SequenceMatcher.ratio&lt;/code&gt; method (except they multiply by 100 and round off so it's a percentage rather than a fraction. The ratio it's calculating is:
&lt;/p&gt;

&lt;p&gt;
\[
ratio = \frac{2M}{T}
\]
&lt;/p&gt;

&lt;p&gt;
Where &lt;i&gt;M&lt;/i&gt; is the number of matching elements and &lt;i&gt;T&lt;/i&gt; is the total number of elements in both sequences.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sentence_a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"eat more meats"&lt;/span&gt;
&lt;span class="n"&gt;sentence_b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"eat more beats"&lt;/span&gt;
&lt;span class="n"&gt;sentence_c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"beat more beets"&lt;/span&gt;
&lt;span class="n"&gt;matcher&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SequenceMatcher&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sentence_a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sentence_b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;matcher&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ratio&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fuzz&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ratio&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence_a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sentence_b&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fuzz&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ratio&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence_b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sentence_c&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
0.9285714285714286
93
90

&lt;/pre&gt;

&lt;p&gt;
The fuzzywuzzy page I mentioned earlier states that this will work for very short (one word) text or very long text, but not so well for things in between.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga8cacfe" class="outline-4"&gt;
&lt;h4 id="orga8cacfe"&gt;Partial Ratio&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga8cacfe"&gt;
&lt;p&gt;
To get better matches for short-ish text, fuzzywuzzy has a &lt;code&gt;partial_ratio&lt;/code&gt; function. 
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sentence_a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"meaty beaty big and bouncy"&lt;/span&gt;
&lt;span class="n"&gt;sentence_b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"meaty"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fuzz&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ratio&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence_a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sentence_b&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fuzz&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;partial_ratio&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence_a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sentence_b&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
32
100

&lt;/pre&gt;

&lt;p&gt;
The &lt;code&gt;ratio&lt;/code&gt; doesn't handle sub-string matches as well as &lt;code&gt;partial_ratio&lt;/code&gt; does.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org135af13" class="outline-4"&gt;
&lt;h4 id="org135af13"&gt;Token Sort and Token Set&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org135af13"&gt;
&lt;p&gt;
Besides sub-strings, there might be cases where ordering doesn't matter, in which case you can try the &lt;code&gt;token_sort_ratio&lt;/code&gt; or &lt;code&gt;token_set_ratio&lt;/code&gt; functions.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sentence_a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"totally tubular terry"&lt;/span&gt;
&lt;span class="n"&gt;sentence_b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"terry is totally tubular"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fuzz&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ratio&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence_a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sentence_b&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fuzz&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;partial_ratio&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence_a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sentence_b&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fuzz&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;token_sort_ratio&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence_a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sentence_b&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fuzz&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;token_set_ratio&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence_a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sentence_b&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
67
83
93
100

&lt;/pre&gt;

&lt;p&gt;
The &lt;code&gt;token_sort_ratio&lt;/code&gt; sorts the tokens before comparing them, while the &lt;code&gt;token_set_ratio&lt;/code&gt; sorts the intersection of the tokens first and then append the sorted tokens that aren't in both strings before calculating the similarity.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8159f3b" class="outline-4"&gt;
&lt;h4 id="org8159f3b"&gt;Process&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org8159f3b"&gt;
&lt;p&gt;
Finally, you can pass &lt;code&gt;process.extract&lt;/code&gt; a string and a list of strings to compare to that string and it will return them in the order of similarity.
&lt;/p&gt;

&lt;p&gt;
By default this uses &lt;code&gt;fuzz.WRatio&lt;/code&gt; to score the similarity.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fuzz&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;WRatio&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="vm"&gt;__doc__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;

Return a measure of the sequences' similarity between 0 and 100, using different algorithms.

**Steps in the order they occur**

#. Run full_process from utils on both strings
#. Short circuit if this makes either string empty
#. Take the ratio of the two processed strings (fuzz.ratio)
#. Run checks to compare the length of the strings
    * If one of the strings is more than 1.5 times as long as the other
      use partial_ratio comparisons - scale partial results by 0.9
      (this makes sure only full results can return 100)
    * If one of the strings is over 8 times as long as the other
      instead scale by 0.6

#. Run the other ratio functions
    * if using partial ratio functions call partial_ratio,
      partial_token_sort_ratio and partial_token_set_ratio
      scale all of these by the ratio based on length
    * otherwise call token_sort_ratio and token_set_ratio
    * all token based comparisons are scaled by 0.95
      (on top of any partial scalars)

#. Take the highest value from these results
   round it and return it as an integer.

:param s1:
:param s2:
:param force_ascii: Allow only ascii characters
:type force_ascii: bool
:full_process: Process inputs, used here to avoid double processing in extract functions (Default: True)
:return:

&lt;/pre&gt;

&lt;p&gt;
Based on the doc-string, it looks like this one tries to figure out the best metric for you.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;choices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"big bubba"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"hubba bubba"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"rubber baby buggy bubba"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"bubba dubba"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"chubba bubba"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;process&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extract&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'hubba hubba'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;choices&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
[('hubba bubba', 95), ('chubba bubba', 87), ('bubba dubba', 82), ('rubber baby buggy bubba', 68), ('big bubba', 54)]

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;process&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extract&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'hubba hubba'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;choices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;limit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
[('hubba bubba', 95), ('chubba bubba', 87)]

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8207e2f" class="outline-4"&gt;
&lt;h4 id="org8207e2f"&gt;Spell Check&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org8207e2f"&gt;
&lt;p&gt;
Although the fuzzywuzzy page states that their use case was matching the names of shows on different web-sites, it can also be used as a simple spell-checker.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dictionary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"embarras"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"inoculate"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"misspell"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"embaras"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"mispel"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"inocullate"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"babaganoush"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"|Word| Correction| Score|"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"|-+-+-|"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;process&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extract&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;limit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;guess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"|{word}|{guess}|{score}|"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Word&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Correction&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;embaras&lt;/td&gt;
&lt;td class="org-left"&gt;embarras&lt;/td&gt;
&lt;td class="org-right"&gt;93&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;mispel&lt;/td&gt;
&lt;td class="org-left"&gt;misspell&lt;/td&gt;
&lt;td class="org-right"&gt;86&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;inocullate&lt;/td&gt;
&lt;td class="org-left"&gt;inoculate&lt;/td&gt;
&lt;td class="org-right"&gt;95&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;babaganoush&lt;/td&gt;
&lt;td class="org-left"&gt;embarras&lt;/td&gt;
&lt;td class="org-right"&gt;42&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
Looking at the last row you can see one of the limitations of this kind of system - it always returns a match, even though there aren't any close matches, so you probably should check the score when using it.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1604c14" class="outline-3"&gt;
&lt;h3 id="org1604c14"&gt;A Diversion Into JellyFish&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1604c14"&gt;
&lt;p&gt;
&lt;a href="https://jellyfish.readthedocs.io/en/latest"&gt;JellyFish&lt;/a&gt; is another python library that implements distance functions (like the Levenstein Distance that FuzzyWuzzy does, but others as well) as well as &lt;a href="https://www.wikiwand.com/en/Stemming"&gt;stemming&lt;/a&gt; and &lt;a href="https://www.wikiwand.com/en/Phonetic_algorithm"&gt;phonetic encoding&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge88afcf" class="outline-4"&gt;
&lt;h4 id="orge88afcf"&gt;Phonetic Encoding&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge88afcf"&gt;
&lt;p&gt;
Phonetic encoding transforms words into a form that is based on the pronounciaton of the words. Using this should make matching spelling variations using the distance function(s) better.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0f407e2" class="outline-5"&gt;
&lt;h5 id="org0f407e2"&gt;American Soundex&lt;/h5&gt;
&lt;div class="outline-text-5" id="text-org0f407e2"&gt;
&lt;p&gt;
&lt;a href="https://www.wikiwand.com/en/Soundex"&gt;Soundex&lt;/a&gt; was originally patented in 1918 but JellyFish uses a variation called &lt;i&gt;American Soundex&lt;/i&gt; which was created in 1930 by analyzing U.S. census reports. Each encoding consists of a letter followed by three digits. The letter is the first letter of the word and the digits represent an encoding of the remaining consonants (a, e, i, o, u, y, h, and w are removed if they aren't the first letter).
&lt;/p&gt;

&lt;p&gt;
The exact procedure is pretty straight-forward, but the main thing to note is that it always has the same form (you either pad or cut off the coded consonants to get three digits).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;distance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;token_1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;token_2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Callable&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;encoded_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;token_1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;encoded_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;token_2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;distance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fuzz&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ratio&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;encoded_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoded_2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;encoded_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoded_2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;distance&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rupert_robert_rwanda&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;guy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sky&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;score_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="n"&gt;distance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"guy"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"sky"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;glove&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;love&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;score_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;distance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"glove"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"love"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;score_3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;distance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"ate"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"eight"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
	&lt;span class="s2"&gt;"Token 1"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"guy ({guy})"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"glove ({glove})"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"ate ({ate})"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
	&lt;span class="s2"&gt;"Token 2"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"sky ({sky})"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"love ({love})"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"eight ({eight})"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
	&lt;span class="s2"&gt;"Similarity"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;score_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;score_2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;score_3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
	&lt;span class="s2"&gt;"Levenshtein Distance"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;jellyfish&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;levenshtein_distance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;guy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sky&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
				 &lt;span class="n"&gt;jellyfish&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;levenshtein_distance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;glove&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;love&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
				 &lt;span class="n"&gt;jellyfish&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;levenshtein_distance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eight&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
	&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tabulate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"keys"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tablefmt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"orgtbl"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;rupert_robert_rwanda&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;jellyfish&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;soundex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Token 1&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Token 2&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Similarity&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Levenshtein Distance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;guy (G000)&lt;/td&gt;
&lt;td class="org-left"&gt;sky (S000)&lt;/td&gt;
&lt;td class="org-right"&gt;75&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;glove (G410)&lt;/td&gt;
&lt;td class="org-left"&gt;love (L100)&lt;/td&gt;
&lt;td class="org-right"&gt;50&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;ate (A300)&lt;/td&gt;
&lt;td class="org-left"&gt;eight (E230)&lt;/td&gt;
&lt;td class="org-right"&gt;50&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
It's a little hard to interpret these values, but it's interesting that &lt;i&gt;guy&lt;/i&gt; and &lt;i&gt;sky&lt;/i&gt; are so much more similar than &lt;i&gt;glove&lt;/i&gt; and &lt;i&gt;love&lt;/i&gt; and &lt;i&gt;eight&lt;/i&gt; and &lt;i&gt;ate&lt;/i&gt; are.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org71f7248" class="outline-5"&gt;
&lt;h5 id="org71f7248"&gt;Metaphone&lt;/h5&gt;
&lt;div class="outline-text-5" id="text-org71f7248"&gt;
&lt;p&gt;
&lt;a href="https://www.wikiwand.com/en/Metaphone"&gt;Metaphone&lt;/a&gt; was developed in 1990 and improves on Soundex to produce a more accurate encoding.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;rupert_robert_rwanda&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;jellyfish&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;metaphone&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Token 1&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Token 2&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Similarity&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Levenshtein Distance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;guy (K)&lt;/td&gt;
&lt;td class="org-left"&gt;sky (SK)&lt;/td&gt;
&lt;td class="org-right"&gt;67&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;glove (KLF)&lt;/td&gt;
&lt;td class="org-left"&gt;love (LF)&lt;/td&gt;
&lt;td class="org-right"&gt;80&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;ate (AT)&lt;/td&gt;
&lt;td class="org-left"&gt;eight (ET)&lt;/td&gt;
&lt;td class="org-right"&gt;50&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
One interesting thing is that &lt;i&gt;metaphone&lt;/i&gt; changes the letters to make match how it thinks something sounds, rather than using the first letter the way &lt;i&gt;soundex&lt;/i&gt; does.
&lt;/p&gt;

&lt;p&gt;
Metaphone is a &lt;i&gt;little&lt;/i&gt; more interpretable, but interestingly in this case the similarity flips and &lt;i&gt;love&lt;/i&gt; and &lt;i&gt;glove&lt;/i&gt; are rated more similar. Also, in this case they all had a Levenshtein Distance of 1, while their similarity-ratios where quite different (Levenstein Distance is the number of edits you need to transform one sequence to another).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc44eec7" class="outline-5"&gt;
&lt;h5 id="orgc44eec7"&gt;New York State Identification and Intelligence System (NYSIIS)&lt;/h5&gt;
&lt;div class="outline-text-5" id="text-orgc44eec7"&gt;
&lt;p&gt;
The &lt;a href="https://www.wikiwand.com/en/New_York_State_Identification_and_Intelligence_System"&gt;New York State Identification and Intelligence System&lt;/a&gt; is a slightly more accurate (compared to Soundex) encoder that was developed in 1970.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;rupert_robert_rwanda&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;jellyfish&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nysiis&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Token 1&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Token 2&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Similarity&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Levenshtein Distance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;guy (GY)&lt;/td&gt;
&lt;td class="org-left"&gt;sky (SCY)&lt;/td&gt;
&lt;td class="org-right"&gt;40&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;glove (GLAV)&lt;/td&gt;
&lt;td class="org-left"&gt;love (LAV)&lt;/td&gt;
&lt;td class="org-right"&gt;86&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;ate (AT)&lt;/td&gt;
&lt;td class="org-left"&gt;eight (EAGT)&lt;/td&gt;
&lt;td class="org-right"&gt;67&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
This seems even more interpretable than the metaphone encodings, and the gap in the similarities is even greater.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfdd2333" class="outline-5"&gt;
&lt;h5 id="orgfdd2333"&gt;Match Rating Approach&lt;/h5&gt;
&lt;div class="outline-text-5" id="text-orgfdd2333"&gt;
&lt;p&gt;
The final phonetic encoding that jellyfish supports is the &lt;a href="https://www.wikiwand.com/en/Match_rating_approach"&gt;Match Rating Approach&lt;/a&gt; which was developed in 1977 by Western Airlines.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;rupert_robert_rwanda&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;jellyfish&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;match_rating_codex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Token 1&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Token 2&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Similarity&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Levenshtein Distance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;guy (GY)&lt;/td&gt;
&lt;td class="org-left"&gt;sky (SKY)&lt;/td&gt;
&lt;td class="org-right"&gt;40&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;glove (GLV)&lt;/td&gt;
&lt;td class="org-left"&gt;love (LV)&lt;/td&gt;
&lt;td class="org-right"&gt;80&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;ate (AT)&lt;/td&gt;
&lt;td class="org-left"&gt;eight (EGHT)&lt;/td&gt;
&lt;td class="org-right"&gt;33&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
This seems the easiest to read, but strangely it made &lt;i&gt;ate&lt;/i&gt; &lt;i&gt;eight&lt;/i&gt; the least similar out of all the encodings.
&lt;/p&gt;

&lt;p&gt;
Interestingly all the encodings except &lt;i&gt;soundex&lt;/i&gt; found that "glove" and "love" are more similar than "guy" and "sky" which are in turn more similar to each other than "ate" and "eight" are, which is not what I would have thought, given that they sound the same when spoken out loud.
&lt;/p&gt;

&lt;p&gt;
I think as with all things, you'd have to try them out to see how well each does with a particular data set.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9f8571f" class="outline-3"&gt;
&lt;h3 id="org9f8571f"&gt;A Short Diversion Into FlashText&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9f8571f"&gt;
&lt;p&gt;
&lt;a href="https://flashtext.readthedocs.io/en/latest/"&gt;FlashText&lt;/a&gt; is a python module to help find and replace words in very large documents. It doesn't do all the interesting linguistic things that the other code we've been looking at does, but it was built specifically to be very fast so for cases where you have a lot of text you can use it to speed up searches.
&lt;/p&gt;

&lt;p&gt;
I don't really have any large text to test it on, but here's a quick look at how it works. You can search for matching words.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;processor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;KeywordProcessor&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;name_to_replace&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"secret agent"&lt;/span&gt;
&lt;span class="n"&gt;replacement&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"Secret Agent"&lt;/span&gt;
&lt;span class="n"&gt;processor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_keyword&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name_to_replace&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;replacement&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;processor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extract_keywords&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
['Secret Agent', 'Secret Agent', 'Secret Agent', 'Secret Agent', 'Secret Agent', 'Secret Agent', 'Secret Agent', 'Secret Agent', 'Secret Agent', 'Secret Agent', 'Secret Agent', 'Secret Agent', 'Secret Agent', 'Secret Agent', 'Secret Agent', 'Secret Agent', 'Secret Agent', 'Secret Agent', 'Secret Agent']

&lt;/pre&gt;

&lt;p&gt;
You don't have to make a replacement, if you only pass in one term then that's what will be replaced.
&lt;/p&gt;

&lt;p&gt;
You can also make a new string with all the terms replaced.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;processor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_keyword&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"shop"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"store"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;processor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_keyword&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"house"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"hovel"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;replacement&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;processor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace_keywords&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;replacement&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
(his wife was in charge of his brother-in-law.

, The shop was small, and so was the house.  , It was one of those grimy
brick houses which existed in large quantities before the era of
reconstruction dawned upon London.  , The shop was a square box of a place,
with the front glazed in small panes.  )

(his wife was in charge of his brother-in-law.

, The store was small, and so was the hovel.  , It was one of those grimy
brick houses which existed in large quantities before the era of
reconstruction dawned upon London.  , The store was a square box of a place,
with the front glazed in small panes.  )
&lt;/pre&gt;

&lt;p&gt;
Note that since it isn't fuzzy "houses" didn't get matched but "house" did.
&lt;/p&gt;

&lt;p&gt;
Instead of just searching for words you can get their indices in the string as well.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;processor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;KeywordProcessor&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;processor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_keyword&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"secret agent"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;processor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extract_keywords&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;span_info&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
[('secret agent', 41658, 41670), ('secret agent', 92721, 92733), ('secret agent', 218743, 218755), ('secret agent', 218989, 219001), ('secret agent', 221234, 221246), ('secret agent', 233831, 233843), ('secret agent', 236874, 236886), ('secret agent', 302412, 302424), ('secret agent', 303622, 303634), ('secret agent', 350862, 350874), ('secret agent', 383719, 383731), ('secret agent', 384291, 384303), ('secret agent', 393206, 393218), ('secret agent', 400450, 400462), ('secret agent', 414087, 414099), ('secret agent', 414319, 414331), ('secret agent', 440086, 440098), ('secret agent', 481986, 481998), ('secret agent', 520737, 520749)]

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;92721&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;31&lt;/span&gt;
&lt;span class="n"&gt;end&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;92733&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
more completely than that of a secret agent of police. 

&lt;/pre&gt;

&lt;p&gt;
Note that the matching isn't case-sensitive, so the previous search mathches "SECRET AGENT", "Secret Agent", etc. although you can make it case-sensitive by passing in the &lt;code&gt;case_sensitive=True&lt;/code&gt; argument to the constructor.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org515abb4" class="outline-2"&gt;
&lt;h2 id="org515abb4"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org515abb4"&gt;
&lt;p&gt;
Despite the name this was really a look at three-ish libraries to help with tokenization, lemmatizing, fuzzy string matching, and quick string searching and replacin.g
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org08a394e" class="outline-3"&gt;
&lt;h3 id="org08a394e"&gt;Reference&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org08a394e"&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Kasliwal N. Natural language processing with Python quick start guide: going from a Python developer to an effective natural language processing engineer [Internet]. 2018 [cited 2019 May 18]. Available from: &lt;a href="http://proquest.safaribooksonline.com/?fpi=9781789130386"&gt;http://proquest.safaribooksonline.com/?fpi=9781789130386&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>data</category><category>tidying</category><guid>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/data/tidying-data/</guid><pubDate>Mon, 20 May 2019 20:15:38 GMT</pubDate></item><item><title>Newsgroups Example</title><link>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/newsgroups-example/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/newsgroups-example/#orgb1df323"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/newsgroups-example/#orga8c4ba8"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/newsgroups-example/#orgb703285"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/newsgroups-example/#org0d7bf84"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/newsgroups-example/#org9696917"&gt;Loading the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/newsgroups-example/#org47852fc"&gt;Setting Up The Test Set&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/newsgroups-example/#org4dbf5b8"&gt;The Document Term Matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/newsgroups-example/#org92ddeba"&gt;Term-Frequency/Inverse Document Frequency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/newsgroups-example/#orga536ad7"&gt;A Logistic Regression Pipeline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/newsgroups-example/#org29f4fe1"&gt;Cross Validation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/newsgroups-example/#orgf95d1d8"&gt;Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/newsgroups-example/#org8803884"&gt;A Confusion Matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/newsgroups-example/#org3b3724e"&gt;End&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/newsgroups-example/#org6a76866"&gt;Original Source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb1df323" class="outline-2"&gt;
&lt;h2 id="orgb1df323"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgb1df323"&gt;
&lt;p&gt;
This is going to be a look at the &lt;a href="https://archive.ics.uci.edu/ml/datasets/twenty+newsgroups"&gt;Twenty Newsgroups&lt;/a&gt; dataset which has posts from twenty newsgroups classified by the newsgroup that they were in. It will be sort of a mad-dash through a simple pipeline to create a model that can classify them.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga8c4ba8" class="outline-3"&gt;
&lt;h3 id="orga8c4ba8"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga8c4ba8"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb94fdb7" class="outline-4"&gt;
&lt;h4 id="orgb94fdb7"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgb94fdb7"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;functools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;partial&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org48c1cb7" class="outline-4"&gt;
&lt;h4 id="org48c1cb7"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org48c1cb7"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;dotenv&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_dotenv&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;fetch_20newsgroups&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;CountVectorizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;TfidfTransformer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;TfidfVectorizer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;confusion_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;f1_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;precision_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;recall_score&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.model_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;cross_val_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.pipeline&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tabulate&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;tabulate&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;holoviews&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;hvplot.pandas&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org386375a" class="outline-4"&gt;
&lt;h4 id="org386375a"&gt;My Stuff&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org386375a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;graeae.tables&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;CountPercentage&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;graeae.timers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Timer&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;graeae.visualization&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;EmbedHoloview&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb703285" class="outline-3"&gt;
&lt;h3 id="orgb703285"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb703285"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd608c42" class="outline-4"&gt;
&lt;h4 id="orgd608c42"&gt;Load the Dotenv&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgd608c42"&gt;
&lt;p&gt;
This loads some extra environment variables, in particular the path to the sklearn-data folder.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dotenv_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"~/.env"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expanduser&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;load_dotenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dotenv_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7ac52c2" class="outline-4"&gt;
&lt;h4 id="org7ac52c2"&gt;The Timer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org7ac52c2"&gt;
&lt;p&gt;
This is an object to keep track of how long things take.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;TIMER&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Timer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org21d170a" class="outline-4"&gt;
&lt;h4 id="org21d170a"&gt;The Table&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org21d170a"&gt;
&lt;p&gt;
This is a helper to print org-tables from data-frames.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;TABLE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tabulate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tablefmt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"orgtbl"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"keys"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		&lt;span class="n"&gt;showindex&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"false"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6209732" class="outline-4"&gt;
&lt;h4 id="org6209732"&gt;Plotting&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org6209732"&gt;
&lt;p&gt;
This helps save the plots to the right folder for nikola.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;SLUG&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"newsgroups-example"&lt;/span&gt;
&lt;span class="n"&gt;Embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EmbedHoloview&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;folder_path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"../../files/posts/nlp/"&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;SLUG&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;holoviews&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extension&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"bokeh"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0d7bf84" class="outline-2"&gt;
&lt;h2 id="org0d7bf84"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0d7bf84"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9696917" class="outline-3"&gt;
&lt;h3 id="org9696917"&gt;Loading the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9696917"&gt;
&lt;p&gt;
We're going to be using the &lt;a href="https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html"&gt;20 Newsgroups dataset&lt;/a&gt; that you can download using sklearn's &lt;a href="https://scikit-learn.org/0.19/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups"&gt;fetch_20newsgroups&lt;/a&gt; function. By default it downloads it to a folder named &lt;code&gt;scikit_learn_data&lt;/code&gt; in your home directory, but since &lt;i&gt;everybody&lt;/i&gt; seems to want to add folders to my home directory I prefer to put it in a hidden folder so I'm not always staring at all these folders. To make it more likely that I'll remember the right folder name I put it in an environment variable.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"SKLEARN"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expanduser&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;TIMER&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fetch_20newsgroups&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_home&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-05-18 10:39:58,412 graeae.timers.timer start: Started: 2019-05-18 10:39:58.412167
Downloading 20news dataset. This may take a few minutes.
Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)
2019-05-18 10:40:33,263 graeae.timers.timer end: Ended: 2019-05-18 10:40:33.263259
2019-05-18 10:40:33,263 graeae.timers.timer end: Elapsed: 0:00:34.851092

&lt;/pre&gt;

&lt;p&gt;
The &lt;code&gt;dataset&lt;/code&gt; is an sklearn bunch - a dictionary-like object (but like pandas you can also use dot-notation to access the values).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;" - {key}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;data&lt;/li&gt;
&lt;li&gt;filenames&lt;/li&gt;
&lt;li&gt;target_names&lt;/li&gt;
&lt;li&gt;target&lt;/li&gt;
&lt;li&gt;DESCR&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
The &lt;code&gt;data&lt;/code&gt; is the inputs and the &lt;code&gt;target&lt;/code&gt; is the labels for each input.
&lt;/p&gt;

&lt;p&gt;
The description is really long, but we can look at part of it to get some idea of what's in the data-set.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"#+begin_src rst"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DESCR&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;1085&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"#+end_src"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;.. _20newsgroups_dataset:

The 20 newsgroups text dataset
------------------------------

The 20 newsgroups dataset comprises around 18000 newsgroups posts on
20 topics split in two subsets: one for training (or development)
and the other one for testing (or for performance evaluation). The split
between the train and test set is based upon a messages posted before
and after a specific date.

This module contains two loaders. The first one,
:func:`sklearn.datasets.fetch_20newsgroups`,
returns a list of the raw texts that can be fed to text feature
extractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`
with custom parameters so as to extract feature vectors.
The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,
returns ready-to-use features, i.e., it is not necessary to use a feature
extractor.

**Data Set Characteristics:**

    =================   ==========
    Classes                     20
    Samples total            18846
    Dimensionality               1
    Features                  text
    =================   ==========
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
So, reading the blob the first thing to notice is that they already split the dataset into training and testing sets - even though I didn't specify anything all I really got was the training set.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org47852fc" class="outline-3"&gt;
&lt;h3 id="org47852fc"&gt;Setting Up The Test Set&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org47852fc"&gt;
&lt;p&gt;
Normally I would use sklearn's &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"&gt;train_test_split&lt;/a&gt; to split the data set up, but since they set it up so that you have to download the test set separately, I guess I'll go with that.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;test_set&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fetch_20newsgroups&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_home&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;subset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"test"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;
&lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;
&lt;span class="n"&gt;x_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_set&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;
&lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_set&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;
&lt;span class="n"&gt;train_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;test_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;total_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_set&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Training: {train_count:,} "&lt;/span&gt;
      &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"({100 * train_count /total_count:.0f} %)"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Testing: {test_count:,} ({100 * test_count/total_count:.0f} %)"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Training: 11,314 (60 %)
Testing: 7,532 (40 %)

&lt;/pre&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4b32218" class="outline-4"&gt;
&lt;h4 id="org4b32218"&gt;What does an entry look like?&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org4b32218"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
From: joachim@kih.no (joachim lous)
Subject: Re: TIFF: philosophical significance of 42
Organization: Kongsberg Ingeniorhogskole
Lines: 30
NNTP-Posting-Host: samson.kih.no
X-Newsreader: TIN [version 1.1 PL8]

ulrich@galki.toppoint.de wrote:

&amp;gt; According to the TIFF 5.0 Specification, the TIFF "version number"
&amp;gt; (bytes 2-3) 42 has been chosen for its "deep philosophical 
&amp;gt; significance".

&amp;gt; When I first read this, I rotfl. Finally some philosphy in a technical
&amp;gt; spec. But still I wondered what makes 42 so significant.

&amp;gt; Last week, I read the Hitchhikers Guide To The Galaxy, and rotfl the
&amp;gt; second time. (After millions of years of calculation, the second-best
&amp;gt; computer of all time reveals that 42 is the answer to the question
&amp;gt; about life, the universe and everything)

&amp;gt; Is this actually how they picked the number 42?

Yes.

&amp;gt; Does anyone have any  other suggestions where the 42 came from?

I don't know where Douglas Adams took it from, but I'm pretty sure he's
the one who launched it (in the Guide). Since then it's been showing up 
all over the place.

    _______________________________
   / _ L*   /  _  / .    /      _  /_  "One thing is for sure: The sheep
  /  _)    /()(/(/)//)) /_ ()(/_) / /  Is NOT a creature of the earth."
 / \_)~  (/ Joachim@kih.no       / /     
/_______________________________/ / -The back-masking on 'Haaden II'
 /_______________________________/  from 'Exposure' by Robert Fripp.

&lt;/pre&gt;

&lt;p&gt;
Looks like there's a lot of noise in these things.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org961a725" class="outline-4"&gt;
&lt;h4 id="org961a725"&gt;How are the groups distributed?&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org961a725"&gt;
&lt;p&gt;
The labels are numbers representing which Newsgroup each of the documents came from, but they also give us a translation in the form of the &lt;code&gt;dataset.target_names&lt;/code&gt; list so we can take a look at how much of each group is represented.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;)}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;CountPercentage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value_label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Newsgroup"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_count&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Newsgroup&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Count&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Percent (%)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;rec.sport.hockey&lt;/td&gt;
&lt;td class="org-right"&gt;600&lt;/td&gt;
&lt;td class="org-right"&gt;5.30&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;soc.religion.christian&lt;/td&gt;
&lt;td class="org-right"&gt;599&lt;/td&gt;
&lt;td class="org-right"&gt;5.29&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;rec.motorcycles&lt;/td&gt;
&lt;td class="org-right"&gt;598&lt;/td&gt;
&lt;td class="org-right"&gt;5.29&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;rec.sport.baseball&lt;/td&gt;
&lt;td class="org-right"&gt;597&lt;/td&gt;
&lt;td class="org-right"&gt;5.28&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;sci.crypt&lt;/td&gt;
&lt;td class="org-right"&gt;595&lt;/td&gt;
&lt;td class="org-right"&gt;5.26&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;rec.autos&lt;/td&gt;
&lt;td class="org-right"&gt;594&lt;/td&gt;
&lt;td class="org-right"&gt;5.25&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;sci.med&lt;/td&gt;
&lt;td class="org-right"&gt;594&lt;/td&gt;
&lt;td class="org-right"&gt;5.25&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;comp.windows.x&lt;/td&gt;
&lt;td class="org-right"&gt;593&lt;/td&gt;
&lt;td class="org-right"&gt;5.24&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;sci.space&lt;/td&gt;
&lt;td class="org-right"&gt;593&lt;/td&gt;
&lt;td class="org-right"&gt;5.24&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;sci.electronics&lt;/td&gt;
&lt;td class="org-right"&gt;591&lt;/td&gt;
&lt;td class="org-right"&gt;5.22&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;comp.os.ms-windows.misc&lt;/td&gt;
&lt;td class="org-right"&gt;591&lt;/td&gt;
&lt;td class="org-right"&gt;5.22&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;comp.sys.ibm.pc.hardware&lt;/td&gt;
&lt;td class="org-right"&gt;590&lt;/td&gt;
&lt;td class="org-right"&gt;5.21&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;misc.forsale&lt;/td&gt;
&lt;td class="org-right"&gt;585&lt;/td&gt;
&lt;td class="org-right"&gt;5.17&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;comp.graphics&lt;/td&gt;
&lt;td class="org-right"&gt;584&lt;/td&gt;
&lt;td class="org-right"&gt;5.16&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;comp.sys.mac.hardware&lt;/td&gt;
&lt;td class="org-right"&gt;578&lt;/td&gt;
&lt;td class="org-right"&gt;5.11&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;talk.politics.mideast&lt;/td&gt;
&lt;td class="org-right"&gt;564&lt;/td&gt;
&lt;td class="org-right"&gt;4.98&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;talk.politics.guns&lt;/td&gt;
&lt;td class="org-right"&gt;546&lt;/td&gt;
&lt;td class="org-right"&gt;4.83&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;alt.atheism&lt;/td&gt;
&lt;td class="org-right"&gt;480&lt;/td&gt;
&lt;td class="org-right"&gt;4.24&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;talk.politics.misc&lt;/td&gt;
&lt;td class="org-right"&gt;465&lt;/td&gt;
&lt;td class="org-right"&gt;4.11&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;talk.religion.misc&lt;/td&gt;
&lt;td class="org-right"&gt;377&lt;/td&gt;
&lt;td class="org-right"&gt;3.33&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
So, there isn't too much of a spread, although religion (other than Christianity and Microsoft Windows) and politics are a little less represented.
&lt;/p&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y_count&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hvplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Newsgroup"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Count"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Newsgroup Counts"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xrotation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;45&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tools&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"crosshair"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"hover"&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;Embed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;file_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"newsgroups_count"&lt;/span&gt;&lt;span class="p"&gt;)()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/newsgroups-example/newsgroups_count.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;

&lt;p&gt;
Actually, now that I plot it it looks like the last three are underepresented, especially when compared to hockey (some kind of Canadian bias?).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4dbf5b8" class="outline-3"&gt;
&lt;h3 id="org4dbf5b8"&gt;The Document Term Matrix&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4dbf5b8"&gt;
&lt;p&gt;
To work with the data-set we need to convert it to some kind of numeric value. In this case I'm going to use sklearn's &lt;a href="https://scikit-learn.org/0.19/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer"&gt;CountVectorizer&lt;/a&gt; to create a matrix where each row represents a document and each column is a term in the &lt;a href="https://www.wikiwand.com/en/Text_corpus"&gt;corpus&lt;/a&gt; (creating a &lt;a href="https://www.wikiwand.com/en/Bag-of-words_model"&gt;Bag of Words&lt;/a&gt;/&lt;a href="https://www.wikiwand.com/en/Document-term_matrix"&gt;Document Term Matrix&lt;/a&gt;) The values are the count of the terms in each document. Sklearn has an alternative download function - &lt;a href="https://scikit-learn.org/0.19/modules/generated/sklearn.datasets.fetch_20newsgroups_vectorized.html#sklearn.datasets.fetch_20newsgroups_vectorized"&gt;fetch_20newsgroups_vectorized&lt;/a&gt; that will download it already vectorized, but since you have to do the conversion yourself in most cases I thought it would be better not to use it.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;vectorizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;CountVectorizer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;TIMER&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;x_train_vectorized&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vectorizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-05-18 16:17:56,875 graeae.timers.timer start: Started: 2019-05-18 16:17:56.875200
2019-05-18 16:17:58,628 graeae.timers.timer end: Ended: 2019-05-18 16:17:58.628334
2019-05-18 16:17:58,628 graeae.timers.timer end: Elapsed: 0:00:01.753134

&lt;/pre&gt;

&lt;p&gt;
That was quicker than I thought it would be - I guess the data set isn't that large.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train_vectorized&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x_train_vectorized&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Documents: {rows:,} Terms: {columns:,}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
&amp;lt;class 'scipy.sparse.csr.csr_matrix'&amp;gt;
Documents: 11,314 Terms: 130,107

&lt;/pre&gt;

&lt;p&gt;
I was going to inspect the matrix, but it's a sparse matrix so you have to convert it to another type to inspect it, and all it would be is a matrix of numbers (term counts) so I'll leave it for some other time.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org92ddeba" class="outline-3"&gt;
&lt;h3 id="org92ddeba"&gt;Term-Frequency/Inverse Document Frequency&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org92ddeba"&gt;
&lt;p&gt;
If we just use the counts (the Term-Frequency (TF)), then the most common word per document will have the highest value, but if a word is spread across all or at least many documents, then even if it's common in a document it probably won't help us distinguish the documents from each other in a meaningful way. To deal with this we can add a penalty (the Inverse-Document-Frequency (IDF) weight) that lowers the value for a term the more common it is among all the documents. Together the two methods are knows as &lt;a href="https://www.wikiwand.com/en/Tf%E2%80%93idf"&gt;TF-IDF.&lt;/a&gt; Here sklearn's &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html"&gt;TfidfTransformer&lt;/a&gt; does the transform for us.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;transformer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TfidfTransformer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;x_train_tfidf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transformer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train_vectorized&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x_train_tfidf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Documents: {rows:,} Terms: {columns:,}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Documents: 11,314 Terms: 130,107

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga536ad7" class="outline-3"&gt;
&lt;h3 id="orga536ad7"&gt;A Logistic Regression Pipeline&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga536ad7"&gt;
&lt;p&gt;
To classify the documents we're going to use &lt;a href="https://www.wikiwand.com/en/Logistic_regression"&gt;Logistic Regression&lt;/a&gt; (also from &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"&gt;sklearn&lt;/a&gt;). In addition, instead of creating the Document Term Matrix in separate steps as above, I'm going to create a &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"&gt;Pipeline&lt;/a&gt; so sklearn can do it in a single call. An sklearn Pipeline takes as all but the last of it's constructor's argument a list of &lt;code&gt;(name, transform)&lt;/code&gt; tuples, where the &lt;code&gt;transform&lt;/code&gt; argument is an object that has a &lt;code&gt;fit_transform&lt;/code&gt; method (like the &lt;code&gt;CountVectorizer&lt;/code&gt; we saw earlier). The last of argument of the constructor is the model that you are going to fit on the data that the pipeline has transformed. If you don't want to customize the names there's a &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline"&gt;make_pipeline&lt;/a&gt; function that just takes the transformer and model instances as arguments and automatically names them for you, returning the pipeline as its output.
&lt;/p&gt;

&lt;p&gt;
I used the &lt;code&gt;CountVectorizer&lt;/code&gt; followed by the &lt;code&gt;TfidfTransformer&lt;/code&gt; to show them separately, but sklearn actually has a &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"&gt;TfidfVectorizer&lt;/a&gt; class that does both of them for you so I'll use that here.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;&lt;span class="p"&gt;([(&lt;/span&gt;&lt;span class="s2"&gt;"TF-IDF"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;TfidfVectorizer&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt; 
		  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Logistic Regression"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
		      &lt;span class="n"&gt;solver&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"lbfgs"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		      &lt;span class="n"&gt;multi_class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"multinomial"&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
I'm using the defaults for Logistic Regression in general, but it will output a lot of warnings if you don't set the solver and multi-class. I'm using the &lt;a href="https://www.wikiwand.com/en/Limited-memory_BFGS"&gt;Limited-Memory Broyden-Fletcher-Goldfarb-Shanno&lt;/a&gt; algorithm for the solver and setting the &lt;code&gt;multi_class&lt;/code&gt; to "multinomial" (setting it to "auto" would do the same thing since there is more than one document-classification, rather than being a binary classification problem).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org29f4fe1" class="outline-3"&gt;
&lt;h3 id="org29f4fe1"&gt;Cross Validation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org29f4fe1"&gt;
&lt;p&gt;
Now we can do some cross validation.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;TIMER&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cross_val_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
			     &lt;span class="n"&gt;scoring&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"f1_macro"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-05-18 16:18:23,392 graeae.timers.timer start: Started: 2019-05-18 16:18:23.392913
2019-05-18 16:22:00,933 graeae.timers.timer end: Ended: 2019-05-18 16:22:00.933407
2019-05-18 16:22:00,934 graeae.timers.timer end: Elapsed: 0:03:37.540494

&lt;/pre&gt;

&lt;p&gt;
So this took a little bit longer than I thought it might, although it wasn't too long.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;description&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;describe&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Statistic"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Value"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TABLE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Statistic&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;count&lt;/td&gt;
&lt;td class="org-right"&gt;7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;mean&lt;/td&gt;
&lt;td class="org-right"&gt;0.895705&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;std&lt;/td&gt;
&lt;td class="org-right"&gt;0.00459816&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;min&lt;/td&gt;
&lt;td class="org-right"&gt;0.88992&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;25%&lt;/td&gt;
&lt;td class="org-right"&gt;0.892157&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;50%&lt;/td&gt;
&lt;td class="org-right"&gt;0.89517&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;75%&lt;/td&gt;
&lt;td class="org-right"&gt;0.900081&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;max&lt;/td&gt;
&lt;td class="org-right"&gt;0.90037&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
It looks like even with the default parameters the model got a mean/median &lt;a href="https://www.wikiwand.com/en/F1_score"&gt;F1 Score&lt;/a&gt; of 90 %, with a standard deviation of 0.0046, so the variance is pretty low.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf95d1d8" class="outline-3"&gt;
&lt;h3 id="orgf95d1d8"&gt;Testing&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf95d1d8"&gt;
&lt;p&gt;
Let's try fitting it to the whole training set and see how it does.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;TIMER&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;fitted&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-05-18 16:23:58,703 graeae.timers.timer start: Started: 2019-05-18 16:23:58.703461
2019-05-18 16:24:33,056 graeae.timers.timer end: Ended: 2019-05-18 16:24:33.056510
2019-05-18 16:24:33,057 graeae.timers.timer end: Elapsed: 0:00:34.353049

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fitted&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"F1 Score: {f1_score(y_test, predictions, average='weighted'):.2f}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
F1 Score: 0.83

&lt;/pre&gt;

&lt;p&gt;
The F1 score in the test-set was lower than the maximum score for our cross-validation checks, so it may have over-fit the training set - or, since they used time to split the sets, the training set might not really represent the test set.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;"Precision: "&lt;/span&gt;
    &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"{precision_score(y_test, predictions, average='weighted'):.2f}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;"Recall: "&lt;/span&gt;
    &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"{recall_score(y_test, predictions, average='weighted'):.2f}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Precision: 0.83
Recall: 0.83

&lt;/pre&gt;

&lt;p&gt;
The precision and recall are the same as the F1 score, so it isn't better at either (or it's good at both, your pick).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8803884" class="outline-3"&gt;
&lt;h3 id="org8803884"&gt;A Confusion Matrix&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8803884"&gt;
&lt;p&gt;
Having a single value like the F1 score helps us evaluate the model, but using a &lt;a href="https://www.wikiwand.com/en/Confusion_matrix"&gt;Confusion Matrix&lt;/a&gt; (via sklearn's &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"&gt;confusion_matrix&lt;/a&gt; function) will help us understand how the model did a little more.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;confusion&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;confusion_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;confusion&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;confusion&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
(20, 20)
[[237   2   0   0   1]
 [  1 307  14   8   8]
 [  2  21 289  34  12]
 [  0  13  23 284  21]
 [  0   5   6  22 319]]

&lt;/pre&gt;

&lt;p&gt;
The confusion matrix has twenty rows and columns. The rows represent what classification our model predicted and the columns what they actually were - the diagonal is the count of the correctly classified documents. We could just print out the twenty by twenty matrix, but why not plot it instead?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;holoviews&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HeatMap&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			  &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			  &lt;span class="n"&gt;confusion&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;options&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
					      &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
					      &lt;span class="n"&gt;xrotation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;45&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
					      &lt;span class="n"&gt;tools&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"hover"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
					      &lt;span class="n"&gt;colorbar&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
					      &lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Confusion Matrix"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Embed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;file_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"confusion_matrix"&lt;/span&gt;&lt;span class="p"&gt;)()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/newsgroups-example/confusion_matrix.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;

&lt;p&gt;
&lt;b&gt;Note To Future Self:&lt;/b&gt; If you don't set the height and width holoviews will collapse the heatmap into an unviewable point, with only the axes visible (nice library but their documentation is &lt;i&gt;horrible&lt;/i&gt;).
&lt;/p&gt;

&lt;p&gt;
Surprisingly, HoloViews rotated the matrix, but it looks like the values are the same.
&lt;/p&gt;

&lt;p&gt;
As you might expect, &lt;i&gt;talk.religion.misc&lt;/i&gt;, &lt;i&gt;talk.politics.misc&lt;/i&gt;, and &lt;i&gt;alt.atheism&lt;/i&gt;, the three least represented groups in our training set have the least "heat". Surprisingly, &lt;i&gt;rec.sport.baseball&lt;/i&gt; is the deepest red (has the highest value) while it is eleventh in the training set, and hockey, which is the most represented in the training set is sort of light. Maybe I should have tried to match the distributions when I did the train-test set split.
&lt;/p&gt;

&lt;p&gt;
Anyway, since the groups aren't equally represented, the matches probably aren't as interesting as the misses.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;confusion&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fill_diagonal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;most_confused&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;confusion&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Largest Confusion: {most_confused}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Largest Confusion: 95

&lt;/pre&gt;

&lt;p&gt;
The largest confusion was for &lt;i&gt;talk.political.guns&lt;/i&gt; being confused for &lt;i&gt;talk.political.misc&lt;/i&gt;, which doesn't seem that surprising, since guns are miscellaneously political. Let's try a top-five.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;top_five&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;reversed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;confusion&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;])))[:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;top_five&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;" - {value}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;95&lt;/li&gt;
&lt;li&gt;43&lt;/li&gt;
&lt;li&gt;41&lt;/li&gt;
&lt;li&gt;39&lt;/li&gt;
&lt;li&gt;38&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
The remainders in the top five:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;i&gt;comp.graphics&lt;/i&gt; was mistaken for &lt;i&gt;comp.windows.x&lt;/i&gt; (43)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;soc.religion.christian&lt;/i&gt; was mistaken for &lt;i&gt;talk.religion.misc&lt;/i&gt; (41)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;talk.religion.misc&lt;/i&gt; was mistaken for &lt;i&gt;alt.atheism&lt;/i&gt; (39)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;comp.windows.x&lt;/i&gt; was mistaken for &lt;i&gt;comp.os.ms-windows.misc&lt;/i&gt; (38)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
In the dataset (as in life) the most confusisng seemed to be religion, politics, and computers, while the easiest to classify were sports and motorcycles (the rec categories). The &lt;i&gt;sci&lt;/i&gt; categories did all right as well, although not as well as &lt;i&gt;rec&lt;/i&gt; oddly (since I would have assumed they would have more identifying jargon).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3b3724e" class="outline-2"&gt;
&lt;h2 id="org3b3724e"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3b3724e"&gt;
&lt;p&gt;
So here we have a basic walk-through using sklearn to model a document classifier built with Logistic Regression. One of the nice things about logistic regression is that the weights tell you which variables are the most important, but I've never tried that with documents so I don't know how to do that here. There are many improvements that could be made (would have to be made) if this were an attempt to make a real model, but the simplicity of the steps shows how much you can do with off-the-shelf software. Eighty-three percent isn't perfect, but it's pretty good.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6a76866" class="outline-3"&gt;
&lt;h3 id="org6a76866"&gt;Original Source&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6a76866"&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Kasliwal N. Natural language processing with Python quick start guide: going from a Python developer to an effective natural language processing engineer [Internet]. 2018 [cited 2019 May 18]. Available from: &lt;a href="http://proquest.safaribooksonline.com/?fpi=9781789130386"&gt;http://proquest.safaribooksonline.com/?fpi=9781789130386&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>nlp</category><category>walk-through</category><guid>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/newsgroups-example/</guid><pubDate>Thu, 16 May 2019 19:41:32 GMT</pubDate></item><item><title>The Vector Space Model</title><link>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#orgc1aefd5"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#orgb32e3f7"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#org4214af9"&gt;The Query&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#orga562cf5"&gt;The Document&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#orgdd03b34"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#org7a1c741"&gt;The Bit Vector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#org76c95d6"&gt;Term Frequency Vector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#orgf927758"&gt;Inverse Document Frequency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#org6daf1f8"&gt;Ranking with TF-IDF Weighting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#orgb61056b"&gt;Okapi BM25&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#org89a66c3"&gt;Document Length Normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#org3dd934e"&gt;The State of the Art&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#org5ef4ccd"&gt;End&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/#orgb0efdd2"&gt;VSM Improvements&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc1aefd5" class="outline-2"&gt;
&lt;h2 id="orgc1aefd5"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc1aefd5"&gt;
&lt;p&gt;
This is a brief write-up of some notes I took on the Vector Space Model (VSM) used to rank documents by relevancy to keywords in a query. The query is represented as a vector with a 1 for each of the search terms and, in the simplest case, the vector to determine relevance will hold a 1 in each place where the document has a word that matches a query term. This might be easier with an example.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb32e3f7" class="outline-3"&gt;
&lt;h3 id="orgb32e3f7"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb32e3f7"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# python
from collections import Counter
from functools import partial
from pathlib import Path

# pypi
from bokeh.models import HoverTool
import holoviews
import hvplot.pandas
import numpy
import pandas

# my stuff
from graeae.visualization import EmbedHoloview
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
For plotting.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;holoviews.extension("bokeh")
SLUG = "the-vector-space-model/"
output = Path("../../files/posts/text_mining")/SLUG
Embed = partial(EmbedHoloview, folder_path=output)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4214af9" class="outline-3"&gt;
&lt;h3 id="org4214af9"&gt;The Query&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4214af9"&gt;
&lt;p&gt;
The user made a query with some key words which is normally represented by a vector with the count of each term as the values.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;terms = set("movie horror blood".split())
query = numpy.ones(len(terms))
print(terms)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
{'movie', 'horror', 'blood'}

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga562cf5" class="outline-3"&gt;
&lt;h3 id="orga562cf5"&gt;The Document&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga562cf5"&gt;
&lt;p&gt;
We want to retrieve the most relevant documents from a set of documents, which I'll represent as lists of words.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;document_1 = "the movie was about workers at a blood bank".split()
document_2 = "a movie about blood blood and more blood".split()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgdd03b34" class="outline-2"&gt;
&lt;h2 id="orgdd03b34"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgdd03b34"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7a1c741" class="outline-3"&gt;
&lt;h3 id="org7a1c741"&gt;The Bit Vector&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7a1c741"&gt;
&lt;p&gt;
To represent similarity I'll use a vector with ones where there is a word match in the document.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def check_document(document: list, terms: set) -&amp;gt; None:
    print(f"Document: {document}")
    matches = []
    matched_words = []
    for word in terms:
	matched = 1 if word in document else 0
	if matched:
	    matched_words.append(word)
	matches.append(matched)
    matches = numpy.array(matches)
    print(f"Matched Words: {matched_words}")
    print(f"Relevance: {matches.dot(query)}")
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;check_document(document_1, terms)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Document: ['the', 'movie', 'was', 'about', 'workers', 'at', 'a', 'blood', 'bank']
Matched Words: ['movie', 'blood']
Relevance: 2.0

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;check_document(document_2, terms)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Document: ['a', 'movie', 'about', 'blood', 'blood', 'and', 'more', 'blood']
Matched Words: ['movie', 'blood']
Relevance: 2.0

&lt;/pre&gt;

&lt;p&gt;
So we con see one problem with our method right off the bat, in that repeated terms don't increase the relevance so both our documents have the same score, even though one mentioned blood more often.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org76c95d6" class="outline-3"&gt;
&lt;h3 id="org76c95d6"&gt;Term Frequency Vector&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org76c95d6"&gt;
&lt;p&gt;
One approach to fix our lack of giving weight to more occurences of a keyword is to use counts, Term Frequency (TF), instead just a 0 or 1 in our vector.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def counts(document: list, terms: set) -&amp;gt; None:
    print(f"Document: {document}")
    count = Counter()
    for word in document:
	if word in terms:
	    count[word] += 1

    matches = numpy.array([count[term] for term in terms])
    print(f"Matched Words: {count}")
    print(f"Relevance: {matches.dot(query)}")
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;counts(document_1, terms)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Document: ['the', 'movie', 'was', 'about', 'workers', 'at', 'a', 'blood', 'bank']
Matched Words: Counter({'movie': 1, 'blood': 1})
Relevance: 2.0

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;counts(document_2, terms)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Document: ['a', 'movie', 'about', 'blood', 'blood', 'and', 'more', 'blood']
Matched Words: Counter({'blood': 3, 'movie': 1})
Relevance: 4.0

&lt;/pre&gt;

&lt;p&gt;
Now the extra mentions of 'blood' make it seem document 2 seem more relevant to us. What happens, though, if one of the terms is one that appears often in documents with different subjects?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;terms = "movie about blood".split()
document_3 = "This movie was not about running kites as the title implied but was rather about some kind of heart warming story meant to make the heart swoon but just about made my blood boil".split()
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;counts(document_3, terms)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Document: ['This', 'movie', 'was', 'not', 'about', 'running', 'kites', 'as', 'the', 'title', 'implied', 'but', 'was', 'rather', 'about', 'some', 'kind', 'of', 'heart', 'warming', 'story', 'meant', 'to', 'make', 'the', 'heart', 'swoon', 'but', 'just', 'about', 'made', 'my', 'blood', 'boil']
Matched Words: Counter({'about': 3, 'movie': 1, 'blood': 1})
Relevance: 5.0

&lt;/pre&gt;

&lt;p&gt;
My example is a little convoluted, but the point is that the most common words aren't necessarily the most helpful ones.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf927758" class="outline-3"&gt;
&lt;h3 id="orgf927758"&gt;Inverse Document Frequency&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf927758"&gt;
&lt;p&gt;
This method seeks to overcome the problem of words that are too common by reducing the weight of a term the more common it is.
&lt;/p&gt;

&lt;p&gt;
\[
IDF(w) = \log \left(\frac{M + 1}{k} \right)
\]
&lt;/p&gt;

&lt;p&gt;
Where &lt;i&gt;M&lt;/i&gt; is the number of documents in the collection, &lt;i&gt;w&lt;/i&gt; is the word (or term) that we are checking, and &lt;i&gt;k&lt;/i&gt; is the number of documents containing &lt;i&gt;w&lt;/i&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hover = HoverTool(
    tooltips=[
	("k", "@k"),
	("IDF(w)", "@idf")
    ],
    mode="vline",
)
M = 1000
k = numpy.linspace(1, M)
idf = numpy.log((M + 1)/k)
data = holoviews.Table((k, idf), "k", ("idf", "IDF(w)"))
plot = holoviews.Curve(data).opts(
    tools=[hover],
    height=800,
    width=1000,
    title="Inverse Document Frequency (IDF) vs Document Frequency (k)")
Embed(plot=plot, file_name="inverse_document_frequency")()
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/inverse_document_frequency.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;

&lt;p&gt;
You can see that as the number of documents with the word in it (&lt;i&gt;k&lt;/i&gt;) goes up, the weight it gets (&lt;i&gt;IDF&lt;/i&gt;) goes down until it reaches zero when all the documents have the word in it (&lt;i&gt;log(1)&lt;/i&gt; equals 0). We have codified the notion that if a word is &lt;i&gt;too&lt;/i&gt; common, then it is less important to relevance.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6daf1f8" class="outline-3"&gt;
&lt;h3 id="org6daf1f8"&gt;Ranking with TF-IDF Weighting&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6daf1f8"&gt;
&lt;p&gt;
Okay, so we hav Term-Frequency as our basic measure of relevancy, and we have this notion that the more common a word is, the less important it is to relevancy, embodied by Inverse Document Frequency (IDF), how do we use them? Like this.
&lt;/p&gt;

\begin{align}
f(q,d) &amp;amp;= \sum_{i=1}^N x_i y_i\\
       &amp;amp;= \sum_{w \in q \cap d} c(w,q) \cdot c(w, d) \log \frac{M+1}{df(w)}\\
\end{align}

&lt;p&gt;
So, let's unpack this a little.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;\(f(q, d)\) is the function to calculate the relevance of a document (&lt;i&gt;d&lt;/i&gt;) to a query (&lt;i&gt;q&lt;/i&gt;)&lt;/li&gt;
&lt;li&gt;\(w \in q \cap d\) means a word (\(w\)) in the intersection of the words in the query (\(q\)) and the words in this document (\(d\))&lt;/li&gt;
&lt;li&gt;\(c(w, q)\) is the count of the number of times this word is in the query&lt;/li&gt;
&lt;li&gt;\(c(w, d)\) is the count of the number of times this word is in the document&lt;/li&gt;
&lt;li&gt;\(M\) is the count of all documents&lt;/li&gt;
&lt;li&gt;\(df(w)\) is the number of documents with the word (document frequency of \(w\))&lt;/li&gt;
&lt;li&gt;\(\log \frac{M+1}{df(w)}\) is the &lt;i&gt;Inverse Document Frequency&lt;/i&gt; of word \(w\)&lt;/li&gt;
&lt;li&gt;\(c(w, q) \cdot c(w, d)\) is the &lt;i&gt;Term Frequency&lt;/i&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
So the relevance of a document is the sum of the products of the Term Frequency for times the Inverse Document Frequency for each word in the query. &lt;i&gt;Why is this useful?&lt;/i&gt; If a term from the query appears in the document, then it is probably more relevant than a document where it doesn't appear, and if the term appears a second time, then it reinforces the idea of its relevance, but as the term keeps appearing we get less and less assurance that it means something - does the twenty-first occurence tell us much more of its relevance than the twentieth? So we still count all the occurences (\(c(w, d)\)) but multiply it by a discounting factor to offset repetitions (the Inverse Document Frequency weight).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb61056b" class="outline-3"&gt;
&lt;h3 id="orgb61056b"&gt;Okapi BM25&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb61056b"&gt;
&lt;p&gt;
The &lt;a href="https://en.wikipedia.org/wiki/Okapi_BM25"&gt;Okapi BM25&lt;/a&gt; ranking function works in a similar way to TF-IDF, but it uses a silghtly different method. Instead of just the count of the words in a document (&lt;i&gt;c(w, d)&lt;/i&gt;), it also uses the number of documents that have the word (&lt;i&gt;k&lt;/i&gt;).
\[
y = \frac{(k + 1) c(w, d){c(w, d) + k}
\]
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hover = HoverTool(
    tooltips=[
	("c(w,d)", "@c_w_d"),
	("Y", "@y")
    ],
    mode="vline",
)
k = 20
x_limit = 1000
c_w_d = numpy.linspace(1, x_limit)
y = ((k + 1) * c_w_d)/(c_w_d + k)
data = holoviews.Table((c_w_d, y), ("c_w_d", "c(w, d)"), "y")
line = holoviews.HLine(k + 1).opts(color="red", alpha=0.4)
curve = holoviews.Curve(data).opts(
    tools=[hover],
    height=800,
    width=1000,
    title=f"BM25 c(w, d) Transformation (k={k})")
plot = (curve * line).opts(ylim=(0, k + 2))
Embed(plot=plot, file_name="bm25_transformation")()
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/bm25_transformation.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;

&lt;p&gt;
Because of the way it's set up, &lt;i&gt;k + 1&lt;/i&gt; acts as an upper bound on the output. 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org89a66c3" class="outline-3"&gt;
&lt;h3 id="org89a66c3"&gt;Document Length Normalization&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org89a66c3"&gt;
&lt;p&gt;
We have another problem to deal with - longer documents have more words and so are more likely to have the query terms, even if they aren't necessarily relevant. So one thing to do might be do drop really long documents, but some documents are long because the author was logorrheic, and others are long because the have a lot of content and so might be useful if we match them, so we don't want to throw everything away. What we want to do instead is add a penalty for longer documents.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org707be92" class="outline-4"&gt;
&lt;h4 id="org707be92"&gt;Pivoted Length Normalization&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org707be92"&gt;
&lt;p&gt;
This is a method to add a penalty to longer documents and a reward for shorter documents. Here's the equation.
&lt;/p&gt;

&lt;p&gt;
\[
normalizer = 1 -b + b \left( \frac{\textit{document length}}{\textit{average document length}} \right)
\]
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hover = HoverTool(
    tooltips=[
	("Document Length", "@document_length"),
	("Normalization", "@normalizer")
    ],
    mode="vline",
)

x_limit = 1000
average_document_length = x_limit/2
document_length = numpy.linspace(1, x_limit)
b = 0.5
normalizer = 1 - b + b * (document_length/average_document_length)
data = holoviews.Table((document_length, normalizer), 
		       ("document_length", "Document Length"), 
		       ("normalizer", 'Normalization'))
line = holoviews.VLine(average_document_length).opts(color="red", alpha=0.4)
hline = holoviews.HLine(1).opts(color="red", alpha=0.4)
curve = holoviews.Curve(data).opts(
    tools=[hover],
    height=800,
    width=1000,
    title=f"Pivoted Length Nomalization (b={b})")
plot = (curve * line * hline).opts(ylim=(0, 2))
Embed(plot=plot, file_name="pivoted_linear_transformation")()
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/pivoted_linear_transformation.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;

&lt;p&gt;
This term ends up in the denominator of the term-frequency equivalent calculations, so as the word-count goes up, the normalization grows larger, reducing the term-frequency measure, and when the word-count is low it makes the term-frequency measure larger.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3dd934e" class="outline-3"&gt;
&lt;h3 id="org3dd934e"&gt;The State of the Art&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3dd934e"&gt;
&lt;p&gt;
These are the two "best" Vector Space Model ranking functions.
&lt;/p&gt;

&lt;p&gt;
\[
b \in [0, 1]\\
k \in [0, +\inf)
\]
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org643919c" class="outline-4"&gt;
&lt;h4 id="org643919c"&gt;Pivoted Length Normalization&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org643919c"&gt;
&lt;p&gt;
\[
f(q, d) = \sum_{w \in q \cap d} c(w, q) \frac{\ln(1 + ln(1 + c(w, d)))}{1 -b + b\frac{\vert d \vert}{\textit{avg dl}}} \og \frac{M + 1}{df(w)}
\]
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9a2b7d5" class="outline-4"&gt;
&lt;h4 id="org9a2b7d5"&gt;BM25 With Document Length Normalization&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org9a2b7d5"&gt;
&lt;p&gt;
\[
f(q, d) = \sum_{w \in q \cap d} c(w, d) \frac{(k+1) c(w, d)}{c(w, d) + k(1 - b + b \frac{\vert d \vert}{\textit{avg dl}})} \log \frac{M+1}{df(w)}
\]
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5ef4ccd" class="outline-2"&gt;
&lt;h2 id="org5ef4ccd"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5ef4ccd"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb0efdd2" class="outline-3"&gt;
&lt;h3 id="orgb0efdd2"&gt;VSM Improvements&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb0efdd2"&gt;
&lt;p&gt;
These are variations that you can try to improve the effectiveness of Vector Space Model Ranking.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6bec97b" class="outline-4"&gt;
&lt;h4 id="org6bec97b"&gt;Fix the "Dimension"&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org6bec97b"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Use stemmed words, remove stop-words, use ngrams…&lt;/li&gt;
&lt;li&gt;Language and domain-specific tokenization&lt;/li&gt;
&lt;li&gt;&lt;b&gt;&lt;b&gt;In practice&lt;/b&gt;&lt;/b&gt;: Using &lt;i&gt;Bag-of-Words&lt;/i&gt; with phrases is often the best&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org216597c" class="outline-4"&gt;
&lt;h4 id="org216597c"&gt;Changing the Similarity Function&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org216597c"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Cosine Similarity&lt;/li&gt;
&lt;li&gt;Euclidean distance&lt;/li&gt;
&lt;li&gt;&lt;b&gt;&lt;b&gt;In Practice&lt;/b&gt;&lt;/b&gt;: the &lt;i&gt;Dot Product&lt;/i&gt; is still the best function&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge7cd216" class="outline-4"&gt;
&lt;h4 id="orge7cd216"&gt;Use BM25 Variations&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge7cd216"&gt;
&lt;/div&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="orgc21ccd2"&gt;&lt;/a&gt;BM25F&lt;br&gt;
&lt;div class="outline-text-5" id="text-orgc21ccd2"&gt;
&lt;p&gt;
This is for structured documents (the &lt;i&gt;F&lt;/i&gt; stands for "fields" within the document (e.g. title)).
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Uses the "fields", not just the main text&lt;/li&gt;
&lt;li&gt;Combines the counts from all the fields before applying BM25&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="orgd6c094f"&gt;&lt;/a&gt;BM25+&lt;br&gt;
&lt;div class="outline-text-5" id="text-orgd6c094f"&gt;
&lt;p&gt;
This avoids over-penalizing long documents by adding a small constant to the term-frequencies. This has been shown (and proven analytically) to be better than BM25 alone.
&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>lecture</category><category>nlp</category><category>search</category><category>text</category><guid>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/the-vector-space-model/</guid><pubDate>Mon, 06 May 2019 00:13:29 GMT</pubDate></item><item><title>Link Page</title><link>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/link-page/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/link-page/#orge695bc7"&gt;Text Mining&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/link-page/#org0605c54"&gt;Text Retrieval MOOC Link Page&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge695bc7" class="outline-2"&gt;
&lt;h2 id="orge695bc7"&gt;Text Mining&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge695bc7"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0605c54" class="outline-3"&gt;
&lt;h3 id="org0605c54"&gt;&lt;a href="http://sifaka.cs.uiuc.edu/ir/resources/mooctr.html"&gt;Text Retrieval MOOC Link Page&lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>links</category><category>reference</category><guid>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/text_mining/link-page/</guid><pubDate>Sat, 04 May 2019 22:08:18 GMT</pubDate></item><item><title>Natural Language Processing Review</title><link>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#org84661bb"&gt;Departure&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#org73b470d"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#org42ae74b"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#org3a3a2d8"&gt;Initiation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#org7d9ec8f"&gt;Some Vocabulary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#org5eedc76"&gt;Return&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/#org394b493"&gt;Sources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org84661bb" class="outline-2"&gt;
&lt;h2 id="org84661bb"&gt;Departure&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org84661bb"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org73b470d" class="outline-3"&gt;
&lt;h3 id="org73b470d"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org73b470d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from pathlib import Path
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from spacy import displacy
import spacy
from nltk.tokenize import TweetTokenizer
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org42ae74b" class="outline-3"&gt;
&lt;h3 id="org42ae74b"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org42ae74b"&gt;
&lt;p&gt;
I'm using the &lt;code&gt;en&lt;/code&gt; model. Even after you install spacy you have to tell it which model to download.
&lt;/p&gt;

&lt;pre class="example"&gt;
python -m spacy download en
&lt;/pre&gt;

&lt;p&gt;
Once you run that this next block should work - otherwise you'll get an &lt;code&gt;OSError&lt;/code&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;processor = spacy.load("en")
tokenizer = TweetTokenizer()
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SLUG = "natural-language-processing-review"
OUTPUT_FOLDER = Path("../../files/posts/nlp/" + SLUG)
if not OUTPUT_FOLDER.is_dir():
    OUTPUT_FOLDER.mkdir(parents=True)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3a3a2d8" class="outline-2"&gt;
&lt;h2 id="org3a3a2d8"&gt;Initiation&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3a3a2d8"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7d9ec8f" class="outline-3"&gt;
&lt;h3 id="org7d9ec8f"&gt;Some Vocabulary&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7d9ec8f"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1418e2d" class="outline-4"&gt;
&lt;h4 id="org1418e2d"&gt;Natural Language Processing&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1418e2d"&gt;
&lt;p&gt;
&lt;a href="https://www.wikiwand.com/en/Natural_language_processing"&gt;Natural Language Processing&lt;/a&gt; is the computational study of human language with the aim of solving practical problems involving language. There is a related field called &lt;a href="https://www.wikiwand.com/en/Computational_linguistics"&gt;Computational Linguistics&lt;/a&gt; which is concerned more with the modeling of human language but seems related.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1f31c43" class="outline-4"&gt;
&lt;h4 id="org1f31c43"&gt;Corpora&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1f31c43"&gt;
&lt;p&gt;
A text-dataset is called a &lt;i&gt;corpus&lt;/i&gt;, usually made up of texts and metadata associated with each text. Texts are made up of characters grouped into units called &lt;i&gt;tokens&lt;/i&gt;. In English a token is generally a word or number.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf1e23f4" class="outline-5"&gt;
&lt;h5 id="orgf1e23f4"&gt;Tokenization&lt;/h5&gt;
&lt;div class="outline-text-5" id="text-orgf1e23f4"&gt;
&lt;p&gt;
The process of breaking a text up into &lt;i&gt;tokens&lt;/i&gt; is called tokenization. Here's some examples of how to do it with &lt;a href="https://spacy.io/usage/spacy-101"&gt;spacy&lt;/a&gt; and the &lt;a href="https://www.nltk.org"&gt;Natural Language Toolkit (NLTK)&lt;/a&gt;.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;text = "Those are my principles, and if you don't like them... well, I have others.".lower()
print([str(token) for token in processor(text)])
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
['those', 'are', 'my', 'principles', ',', 'and', 'if', 'you', 'do', "n't", 'like', 'them', '...', 'well', ',', 'i', 'have', 'others', '.']

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(tokenizer.tokenize(text))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
['those', 'are', 'my', 'principles', ',', 'and', 'if', 'you', "don't", 'like', 'them', '...', 'well', ',', 'i', 'have', 'others', '.']

&lt;/pre&gt;

&lt;p&gt;
Spacy prefers to break contractions up, while NLTK doesn't, otherwise they treated them pretty much the same way.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;tweet = text + " #GrouchoSaid@morning:-J".lower()
print([str(token) for token in processor(tweet)])
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
['those', 'are', 'my', 'principles', ',', 'and', 'if', 'you', 'do', "n't", 'like', 'them', '...', 'well', ',', 'i', 'have', 'others', '.', '#', 'grouchosaid@morning:-j']

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(tokenizer.tokenize(tweet))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
['those', 'are', 'my', 'principles', ',', 'and', 'if', 'you', "don't", 'like', 'them', '...', 'well', ',', 'i', 'have', 'others', '.', '#grouchosaid', '@morning', ':', '-', 'j']

&lt;/pre&gt;

&lt;p&gt;
In this case, the TweetTokenizer and spacy treated the hash-tag and smiley differently.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1f1bfef" class="outline-4"&gt;
&lt;h4 id="org1f1bfef"&gt;Types&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1f1bfef"&gt;
&lt;p&gt;
&lt;i&gt;Types&lt;/i&gt; are the unique tokens in a corpus. The set of all the types in the corpus is its &lt;i&gt;vocabulary&lt;/i&gt; or &lt;i&gt;lexicon&lt;/i&gt;. 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org13b0655" class="outline-4"&gt;
&lt;h4 id="org13b0655"&gt;Word Classes&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org13b0655"&gt;
&lt;p&gt;
There are two classes of words &lt;i&gt;content words&lt;/i&gt; and &lt;i&gt;stopwords&lt;/i&gt;. Stopwords are there mostly to glue the content words together ("a", "an", "the", etc.) and provide more noise than information.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfc21ad3" class="outline-4"&gt;
&lt;h4 id="orgfc21ad3"&gt;N-Grams&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgfc21ad3"&gt;
&lt;p&gt;
&lt;i&gt;N-grams&lt;/i&gt; are consecutive token-sequences of a fixed length (&lt;i&gt;n&lt;/i&gt;). Common special cases are:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;unigrams: &lt;i&gt;n&lt;/i&gt; = 1&lt;/li&gt;
&lt;li&gt;bigrams: &lt;i&gt;n&lt;/i&gt; = 2&lt;/li&gt;
&lt;li&gt;trigrams: &lt;i&gt;n&lt;/i&gt; = 3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
Although n-grams are generally words in some cases they can be characters - if the suffixes are meaningful, for instance.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8721d6d" class="outline-4"&gt;
&lt;h4 id="org8721d6d"&gt;Lemma&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org8721d6d"&gt;
&lt;p&gt;
&lt;a href="https://www.wikiwand.com/en/Lemma_(morphology)"&gt;Lemmas&lt;/a&gt; are root-forms for words that can have different forms. As an example - &lt;i&gt;go&lt;/i&gt; is the lemma for &lt;i&gt;go&lt;/i&gt;, &lt;i&gt;going&lt;/i&gt;, &lt;i&gt;went&lt;/i&gt;, and &lt;i&gt;gone&lt;/i&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;document = processor("an undefined problem has an infinite number of solutions")
for token in document:
    print("Token: {} -&amp;gt; Lemma: {}".format(token, token.lemma_))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Token: an -&amp;gt; Lemma: an
Token: undefined -&amp;gt; Lemma: undefined
Token: problem -&amp;gt; Lemma: problem
Token: has -&amp;gt; Lemma: have
Token: an -&amp;gt; Lemma: an
Token: infinite -&amp;gt; Lemma: infinite
Token: number -&amp;gt; Lemma: number
Token: of -&amp;gt; Lemma: of
Token: solutions -&amp;gt; Lemma: solution

&lt;/pre&gt;

&lt;p&gt;
There is a related method called &lt;i&gt;stemming&lt;/i&gt; which strips the endings off of words instead of changing to their lemmas. lemmatization is probably preferable, but is a more difficult method compared to stemming.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7481552" class="outline-4"&gt;
&lt;h4 id="org7481552"&gt;Chunking&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org7481552"&gt;
&lt;p&gt;
There are different ways of chunking text instead of just using &lt;i&gt;n-grams&lt;/i&gt;, one way is called &lt;i&gt;chunking&lt;/i&gt; which breaks the text up into phrases.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;document = processor("Mary had a little lamb, its fleece was white as snow.")
for chunk in document.noun_chunks:
    print("{} - {}".format(chunk, chunk.label_))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Mary - NP
a little lamb - NP
its fleece - NP
snow - NP

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div id="outline-container-org5eedc76" class="outline-2"&gt;
&lt;h2 id="org5eedc76"&gt;Return&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5eedc76"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org394b493" class="outline-3"&gt;
&lt;h3 id="org394b493"&gt;Sources&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org394b493"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Rao D, McMahan B. Natural language processing with PyTorch: build intelligent language applications using deep learning. Sebastopol, CA: OReilly Media; 2019.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>nlp</category><category>review</category><guid>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/nlp/natural-language-processing-review/</guid><pubDate>Mon, 08 Apr 2019 00:12:29 GMT</pubDate></item><item><title>Reading List</title><link>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/reference/reading-list/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-orgf0a9335" class="outline-2"&gt;
&lt;h2 id="orgf0a9335"&gt;Books on Natural Language Processing&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf0a9335"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0f89b9a" class="outline-3"&gt;
&lt;h3 id="org0f89b9a"&gt;Rao D, McMahan B. Natural language processing with PyTorch: build intelligent language applications using deep learning. Sebastopol, CA: OReilly Media; 2019.&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>list</category><category>reading</category><guid>https://necromuralist.github.io/Tip-of-the-Dnghu/posts/reference/reading-list/</guid><pubDate>Sun, 07 Apr 2019 23:34:53 GMT</pubDate></item></channel></rss>